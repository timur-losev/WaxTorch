program(1.3)
[buildInfo = dict<string, string>({{"coremlc-component-MIL", "3500.14.1"}, {"coremlc-version", "3500.32.1"}})]
{
    func main<ios18>(tensor<int32, [?, ?]> attention_mask, tensor<int32, [?, ?]> input_ids) [FlexibleShapeInformation = tuple<tuple<string, dict<string, tensor<int32, [?]>>>, tuple<string, dict<string, dict<string, tensor<int32, [?]>>>>>((("DefaultShapes", {{"attention_mask", [1, 32]}, {"input_ids", [1, 32]}}), ("EnumeratedShapes", {{"000187c4", {{"attention_mask", [64, 64]}, {"input_ids", [64, 64]}}}, {"090f2853", {{"attention_mask", [1, 128]}, {"input_ids", [1, 128]}}}, {"0a731900", {{"attention_mask", [1, 256]}, {"input_ids", [1, 256]}}}, {"14e22cc3", {{"attention_mask", [8, 64]}, {"input_ids", [8, 64]}}}, {"155d1fbb", {{"attention_mask", [64, 384]}, {"input_ids", [64, 384]}}}, {"1a2a12fc", {{"attention_mask", [16, 512]}, {"input_ids", [16, 512]}}}, {"22c267bf", {{"attention_mask", [64, 128]}, {"input_ids", [64, 128]}}}, {"24cf7ddf", {{"attention_mask", [64, 32]}, {"input_ids", [64, 32]}}}, {"2721a294", {{"attention_mask", [16, 32]}, {"input_ids", [16, 32]}}}, {"29d5bec5", {{"attention_mask", [32, 256]}, {"input_ids", [32, 256]}}}, {"3a8c1bc2", {{"attention_mask", [32, 512]}, {"input_ids", [32, 512]}}}, {"4b9590f0", {{"attention_mask", [16, 256]}, {"input_ids", [16, 256]}}}, {"50450b3e", {{"attention_mask", [16, 128]}, {"input_ids", [16, 128]}}}, {"517b156a", {{"attention_mask", [8, 512]}, {"input_ids", [8, 512]}}}, {"53dd2036", {{"attention_mask", [32, 384]}, {"input_ids", [32, 384]}}}, {"6526d012", {{"attention_mask", [8, 32]}, {"input_ids", [8, 32]}}}, {"7b263bfe", {{"attention_mask", [1, 512]}, {"input_ids", [1, 512]}}}, {"89020357", {{"attention_mask", [32, 128]}, {"input_ids", [32, 128]}}}, {"92e182a7", {{"attention_mask", [1, 384]}, {"input_ids", [1, 384]}}}, {"964b98d6", {{"attention_mask", [64, 512]}, {"input_ids", [64, 512]}}}, {"a28995a1", {{"attention_mask", [16, 64]}, {"input_ids", [16, 64]}}}, {"a4e8f51c", {{"attention_mask", [32, 32]}, {"input_ids", [32, 32]}}}, {"aa3a1438", {{"attention_mask", [8, 128]}, {"input_ids", [8, 128]}}}, {"b0234bc7", {{"attention_mask", [8, 384]}, {"input_ids", [8, 384]}}}, {"ba32981e", {{"attention_mask", [16, 384]}, {"input_ids", [16, 384]}}}, {"cc37bcd3", {{"attention_mask", [8, 256]}, {"input_ids", [8, 256]}}}, {"d2679837", {{"attention_mask", [64, 256]}, {"input_ids", [64, 256]}}}, {"d8f542e5", {{"attention_mask", [1, 64]}, {"input_ids", [1, 64]}}}, {"e78bf925", {{"attention_mask", [32, 64]}, {"input_ids", [32, 64]}}}, {"f5a604ed", {{"attention_mask", [1, 32]}, {"input_ids", [1, 32]}}}})))] {
            tensor<int32, [1, 512]> model_embeddings_position_ids = const()[name = string("model_embeddings_position_ids"), val = tensor<int32, [1, 512]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(64)))];
            tensor<int32, [?, ?]> input_1 = sub(x = input_ids, y = input_ids)[name = string("sub_0")];
            int32 var_22 = const()[name = string("op_22"), val = int32(32)];
            int32 var_23 = const()[name = string("op_23"), val = int32(12)];
            int32 var_24 = const()[name = string("op_24"), val = int32(-1)];
            int32 var_27 = const()[name = string("op_27"), val = int32(384)];
            tensor<int32, [1]> var_42_axes_0 = const()[name = string("op_42_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [?, 1, ?]> var_42 = expand_dims(axes = var_42_axes_0, x = attention_mask)[name = string("op_42")];
            tensor<int32, [1]> var_43_axes_0 = const()[name = string("op_43_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<int32, [?, 1, 1, ?]> var_43 = expand_dims(axes = var_43_axes_0, x = var_42)[name = string("op_43")];
            fp16 var_29_to_fp16 = const()[name = string("op_29_to_fp16"), val = fp16(0x1p+0)];
            string var_45_to_fp16_dtype_0 = const()[name = string("op_45_to_fp16_dtype_0"), val = string("fp16")];
            tensor<fp16, [?, 1, 1, ?]> var_43_to_fp16 = cast(dtype = var_45_to_fp16_dtype_0, x = var_43)[name = string("cast_77")];
            tensor<fp16, [?, 1, 1, ?]> var_46_cast_fp16 = sub(x = var_29_to_fp16, y = var_43_to_fp16)[name = string("op_46_cast_fp16")];
            fp16 var_47_to_fp16 = const()[name = string("op_47_to_fp16"), val = fp16(-inf)];
            tensor<fp16, [?, 1, 1, ?]> attention_mask_cast_fp16 = mul(x = var_46_cast_fp16, y = var_47_to_fp16)[name = string("attention_mask_cast_fp16")];
            tensor<int32, [2]> var_54_shape = shape(x = input_ids)[name = string("op_54_shape")];
            int32 gather_0_axis_0 = const()[name = string("gather_0_axis_0"), val = int32(0)];
            int32 gather_0_batch_dims_0 = const()[name = string("gather_0_batch_dims_0"), val = int32(0)];
            bool gather_0_validate_indices_0 = const()[name = string("gather_0_validate_indices_0"), val = bool(false)];
            string var_54_shape_to_int16_dtype_0 = const()[name = string("op_54_shape_to_int16_dtype_0"), val = string("int16")];
            uint16 gather_0_indices_0_to_uint16 = const()[name = string("gather_0_indices_0_to_uint16"), val = uint16(1)];
            tensor<int16, [2]> var_54_shape_to_int16 = cast(dtype = var_54_shape_to_int16_dtype_0, x = var_54_shape)[name = string("cast_76")];
            int16 gather_0_cast_uint16 = gather(axis = gather_0_axis_0, batch_dims = gather_0_batch_dims_0, indices = gather_0_indices_0_to_uint16, validate_indices = gather_0_validate_indices_0, x = var_54_shape_to_int16)[name = string("gather_0_cast_uint16")];
            string gather_0_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_0_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_0_values0_0 = const()[name = string("concat_0_values0_0"), val = int32(1)];
            int32 concat_0_axis_0 = const()[name = string("concat_0_axis_0"), val = int32(0)];
            bool concat_0_interleave_0 = const()[name = string("concat_0_interleave_0"), val = bool(false)];
            int32 gather_0_cast_uint16_to_int32 = cast(dtype = gather_0_cast_uint16_to_int32_dtype_0, x = gather_0_cast_uint16)[name = string("cast_75")];
            tensor<int32, [2]> concat_0 = concat(axis = concat_0_axis_0, interleave = concat_0_interleave_0, values = (concat_0_values0_0, gather_0_cast_uint16_to_int32))[name = string("concat_0")];
            tensor<int32, [2]> input_3_begin_0 = const()[name = string("input_3_begin_0"), val = tensor<int32, [2]>([0, 0])];
            tensor<bool, [2]> input_3_end_mask_0 = const()[name = string("input_3_end_mask_0"), val = tensor<bool, [2]>([true, false])];
            tensor<int32, [1, ?]> input_3 = slice_by_index(begin = input_3_begin_0, end = concat_0, end_mask = input_3_end_mask_0, x = model_embeddings_position_ids)[name = string("input_3")];
            int32 inputs_embeds_axis_0 = const()[name = string("inputs_embeds_axis_0"), val = int32(0)];
            int32 inputs_embeds_batch_dims_0 = const()[name = string("inputs_embeds_batch_dims_0"), val = int32(0)];
            bool inputs_embeds_validate_indices_0 = const()[name = string("inputs_embeds_validate_indices_0"), val = bool(false)];
            tensor<fp16, [30522, 384]> model_embeddings_word_embeddings_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [30522, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(2176))), scale = tensor<fp16, [30522, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(11722688))))[name = string("model_embeddings_word_embeddings_weight_to_fp16_quantized")];
            string input_ids_to_uint16_dtype_0 = const()[name = string("input_ids_to_uint16_dtype_0"), val = string("uint16")];
            tensor<uint16, [?, ?]> input_ids_to_uint16 = cast(dtype = input_ids_to_uint16_dtype_0, x = input_ids)[name = string("cast_74")];
            tensor<fp16, [?, ?, 384]> inputs_embeds_cast_fp16_cast_uint16 = gather(axis = inputs_embeds_axis_0, batch_dims = inputs_embeds_batch_dims_0, indices = input_ids_to_uint16, validate_indices = inputs_embeds_validate_indices_0, x = model_embeddings_word_embeddings_weight_to_fp16_quantized)[name = string("inputs_embeds_cast_fp16_cast_uint16")];
            int32 token_type_embeddings_1_axis_0 = const()[name = string("token_type_embeddings_1_axis_0"), val = int32(0)];
            int32 token_type_embeddings_1_batch_dims_0 = const()[name = string("token_type_embeddings_1_batch_dims_0"), val = int32(0)];
            bool token_type_embeddings_1_validate_indices_0 = const()[name = string("token_type_embeddings_1_validate_indices_0"), val = bool(false)];
            tensor<fp16, [2, 384]> model_embeddings_token_type_embeddings_weight_to_fp16 = const()[name = string("model_embeddings_token_type_embeddings_weight_to_fp16"), val = tensor<fp16, [2, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(11783808)))];
            string input_1_to_uint16_dtype_0 = const()[name = string("input_1_to_uint16_dtype_0"), val = string("uint16")];
            tensor<uint16, [?, ?]> input_1_to_uint16 = cast(dtype = input_1_to_uint16_dtype_0, x = input_1)[name = string("cast_73")];
            tensor<fp16, [?, ?, 384]> token_type_embeddings_1_cast_fp16_cast_uint16 = gather(axis = token_type_embeddings_1_axis_0, batch_dims = token_type_embeddings_1_batch_dims_0, indices = input_1_to_uint16, validate_indices = token_type_embeddings_1_validate_indices_0, x = model_embeddings_token_type_embeddings_weight_to_fp16)[name = string("token_type_embeddings_1_cast_fp16_cast_uint16")];
            fp16 quantize_0_scale_0 = const()[name = string("quantize_0_scale_0"), val = fp16(0x1.0ecp-7)];
            string quantize_0_output_dtype_0 = const()[name = string("quantize_0_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_0 = quantize(input = inputs_embeds_cast_fp16_cast_uint16, output_dtype = quantize_0_output_dtype_0, scale = quantize_0_scale_0)[name = string("quantize_0")];
            fp16 quantize_1_scale_0 = const()[name = string("quantize_1_scale_0"), val = fp16(0x1.76cp-9)];
            string quantize_1_output_dtype_0 = const()[name = string("quantize_1_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_1 = quantize(input = token_type_embeddings_1_cast_fp16_cast_uint16, output_dtype = quantize_1_output_dtype_0, scale = quantize_1_scale_0)[name = string("quantize_1")];
            fp16 dequantize_165_scale_0 = const()[name = string("dequantize_165_scale_0"), val = fp16(0x1.72p+1)];
            tensor<fp16, [?, ?, 384]> dequantize_165 = dequantize(input = quantize_0, scale = dequantize_165_scale_0)[name = string("dequantize_165")];
            fp16 dequantize_166_scale_0 = const()[name = string("dequantize_166_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_166 = dequantize(input = quantize_1, scale = dequantize_166_scale_0)[name = string("dequantize_166")];
            tensor<fp16, [?, ?, 384]> embeddings_1_cast_fp16 = add(x = dequantize_165, y = dequantize_166)[name = string("embeddings_1_cast_fp16")];
            int32 position_embeddings_1_axis_0 = const()[name = string("position_embeddings_1_axis_0"), val = int32(0)];
            int32 position_embeddings_1_batch_dims_0 = const()[name = string("position_embeddings_1_batch_dims_0"), val = int32(0)];
            bool position_embeddings_1_validate_indices_0 = const()[name = string("position_embeddings_1_validate_indices_0"), val = bool(false)];
            tensor<fp16, [512, 384]> model_embeddings_position_embeddings_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [512, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(11785408))), scale = tensor<fp16, [512, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(11982080))))[name = string("model_embeddings_position_embeddings_weight_to_fp16_quantized")];
            string input_3_to_uint16_dtype_0 = const()[name = string("input_3_to_uint16_dtype_0"), val = string("uint16")];
            tensor<uint16, [1, ?]> input_3_to_uint16 = cast(dtype = input_3_to_uint16_dtype_0, x = input_3)[name = string("cast_72")];
            tensor<fp16, [1, ?, 384]> position_embeddings_1_cast_fp16_cast_uint16 = gather(axis = position_embeddings_1_axis_0, batch_dims = position_embeddings_1_batch_dims_0, indices = input_3_to_uint16, validate_indices = position_embeddings_1_validate_indices_0, x = model_embeddings_position_embeddings_weight_to_fp16_quantized)[name = string("position_embeddings_1_cast_fp16_cast_uint16")];
            string quantize_2_output_dtype_0 = const()[name = string("quantize_2_output_dtype_0"), val = string("int8")];
            fp16 quantize_0_scale_0_1 = const()[name = string("quantize_0_scale_0_1"), val = fp16(0x1.5a4p+1)];
            tensor<int8, [?, ?, 384]> quantize_0_1 = quantize(input = embeddings_1_cast_fp16, output_dtype = quantize_2_output_dtype_0, scale = quantize_0_scale_0_1)[name = string("quantize_0_1")];
            fp16 quantize_3_scale_0 = const()[name = string("quantize_3_scale_0"), val = fp16(0x1.e38p-7)];
            string quantize_3_output_dtype_0 = const()[name = string("quantize_3_output_dtype_0"), val = string("int8")];
            tensor<int8, [1, ?, 384]> quantize_3 = quantize(input = position_embeddings_1_cast_fp16_cast_uint16, output_dtype = quantize_3_output_dtype_0, scale = quantize_3_scale_0)[name = string("quantize_3")];
            fp16 dequantize_167_scale_0 = const()[name = string("dequantize_167_scale_0"), val = fp16(0x1.0c8p-1)];
            tensor<fp16, [?, ?, 384]> dequantize_167 = dequantize(input = quantize_0_1, scale = dequantize_167_scale_0)[name = string("dequantize_167")];
            fp16 dequantize_168_scale_0 = const()[name = string("dequantize_168_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [1, ?, 384]> dequantize_168 = dequantize(input = quantize_3, scale = dequantize_168_scale_0)[name = string("dequantize_168")];
            tensor<fp16, [?, ?, 384]> input_5_cast_fp16 = add(x = dequantize_167, y = dequantize_168)[name = string("input_5_cast_fp16")];
            string quantize_77_output_dtype_0 = const()[name = string("quantize_77_output_dtype_0"), val = string("int8")];
            fp16 quantize_1_scale_0_1 = const()[name = string("quantize_1_scale_0_1"), val = fp16(0x1.864p+0)];
            tensor<int8, [?, ?, 384]> quantize_1_1 = quantize(input = input_5_cast_fp16, output_dtype = quantize_77_output_dtype_0, scale = quantize_1_scale_0_1)[name = string("quantize_1_1")];
            fp16 dequantize_77_scale_0 = const()[name = string("dequantize_77_scale_0"), val = fp16(0x1.708p-6)];
            tensor<fp16, [?, ?, 384]> dequantize_121 = dequantize(input = quantize_1_1, scale = dequantize_77_scale_0)[name = string("dequantize_121")];
            tensor<int32, [1]> input_7_axes_0 = const()[name = string("input_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_embeddings_LayerNorm_weight_to_fp16 = const()[name = string("model_embeddings_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(11983168)))];
            tensor<fp16, [384]> model_embeddings_LayerNorm_bias_to_fp16 = const()[name = string("model_embeddings_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(11984000)))];
            fp16 var_26_to_fp16 = const()[name = string("op_26_to_fp16"), val = fp16(0x1p-24)];
            tensor<fp16, [?, ?, 384]> input_7_cast_fp16 = layer_norm(axes = input_7_axes_0, beta = model_embeddings_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_embeddings_LayerNorm_weight_to_fp16, x = dequantize_121)[name = string("input_7_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_query_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(11984832))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12132352))))[name = string("model_encoder_layer_0_attention_self_query_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_query_bias_to_fp16 = const()[name = string("model_encoder_layer_0_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12133184)))];
            fp16 quantize_4_scale_0 = const()[name = string("quantize_4_scale_0"), val = fp16(0x1.d5p-5)];
            string quantize_4_output_dtype_0 = const()[name = string("quantize_4_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_4 = quantize(input = input_7_cast_fp16, output_dtype = quantize_4_output_dtype_0, scale = quantize_4_scale_0)[name = string("quantize_4")];
            fp16 dequantize_4_scale_0 = const()[name = string("dequantize_4_scale_0"), val = fp16(0x1.d5p-5)];
            tensor<fp16, [?, ?, 384]> dequantize_4 = dequantize(input = quantize_4, scale = dequantize_4_scale_0)[name = string("dequantize_4")];
            tensor<fp16, [?, ?, 384]> linear_0_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_query_bias_to_fp16, weight = model_encoder_layer_0_attention_self_query_weight_to_fp16_quantized, x = dequantize_4)[name = string("linear_0_cast_fp16")];
            fp16 quantize_78_scale_0 = const()[name = string("quantize_78_scale_0"), val = fp16(0x1.544p-3)];
            string quantize_78_output_dtype_0 = const()[name = string("quantize_78_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_122 = quantize(input = linear_0_cast_fp16, output_dtype = quantize_78_output_dtype_0, scale = quantize_78_scale_0)[name = string("quantize_122")];
            fp16 dequantize_78_scale_0 = const()[name = string("dequantize_78_scale_0"), val = fp16(0x1.544p-3)];
            tensor<fp16, [?, ?, 384]> dequantize_122 = dequantize(input = quantize_122, scale = dequantize_78_scale_0)[name = string("dequantize_122")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_key_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12134016))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12281536))))[name = string("model_encoder_layer_0_attention_self_key_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_key_bias_to_fp16 = const()[name = string("model_encoder_layer_0_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12282368)))];
            tensor<fp16, [?, ?, 384]> linear_1_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_key_bias_to_fp16, weight = model_encoder_layer_0_attention_self_key_weight_to_fp16_quantized, x = dequantize_4)[name = string("linear_1_cast_fp16")];
            fp16 quantize_79_scale_0 = const()[name = string("quantize_79_scale_0"), val = fp16(0x1.f74p-4)];
            string quantize_79_output_dtype_0 = const()[name = string("quantize_79_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_123 = quantize(input = linear_1_cast_fp16, output_dtype = quantize_79_output_dtype_0, scale = quantize_79_scale_0)[name = string("quantize_123")];
            fp16 dequantize_79_scale_0 = const()[name = string("dequantize_79_scale_0"), val = fp16(0x1.f74p-4)];
            tensor<fp16, [?, ?, 384]> dequantize_123 = dequantize(input = quantize_123, scale = dequantize_79_scale_0)[name = string("dequantize_123")];
            tensor<int32, [3]> var_100_shape_cast_fp16 = shape(x = dequantize_123)[name = string("op_100_shape_cast_fp16")];
            int32 gather_1_axis_0 = const()[name = string("gather_1_axis_0"), val = int32(0)];
            int32 gather_1_batch_dims_0 = const()[name = string("gather_1_batch_dims_0"), val = int32(0)];
            bool gather_1_validate_indices_0 = const()[name = string("gather_1_validate_indices_0"), val = bool(false)];
            string var_100_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_100_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_1_indices_0_to_uint16 = const()[name = string("gather_1_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_100_shape_cast_fp16_to_uint16 = cast(dtype = var_100_shape_cast_fp16_to_uint16_dtype_0, x = var_100_shape_cast_fp16)[name = string("cast_71")];
            uint16 gather_1_cast_uint16 = gather(axis = gather_1_axis_0, batch_dims = gather_1_batch_dims_0, indices = gather_1_indices_0_to_uint16, validate_indices = gather_1_validate_indices_0, x = var_100_shape_cast_fp16_to_uint16)[name = string("gather_1_cast_uint16")];
            string gather_1_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_1_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_2_axis_0 = const()[name = string("gather_2_axis_0"), val = int32(0)];
            int32 gather_2_batch_dims_0 = const()[name = string("gather_2_batch_dims_0"), val = int32(0)];
            bool gather_2_validate_indices_0 = const()[name = string("gather_2_validate_indices_0"), val = bool(false)];
            uint16 gather_2_indices_0_to_uint16 = const()[name = string("gather_2_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_2_cast_uint16 = gather(axis = gather_2_axis_0, batch_dims = gather_2_batch_dims_0, indices = gather_2_indices_0_to_uint16, validate_indices = gather_2_validate_indices_0, x = var_100_shape_cast_fp16_to_uint16)[name = string("gather_2_cast_uint16")];
            string gather_2_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_2_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_1_axis_0 = const()[name = string("concat_1_axis_0"), val = int32(0)];
            bool concat_1_interleave_0 = const()[name = string("concat_1_interleave_0"), val = bool(false)];
            int32 gather_2_cast_uint16_to_int32 = cast(dtype = gather_2_cast_uint16_to_int32_dtype_0, x = gather_2_cast_uint16)[name = string("cast_69")];
            int32 gather_1_cast_uint16_to_int32 = cast(dtype = gather_1_cast_uint16_to_int32_dtype_0, x = gather_1_cast_uint16)[name = string("cast_70")];
            tensor<int32, [4]> concat_1 = concat(axis = concat_1_axis_0, interleave = concat_1_interleave_0, values = (gather_1_cast_uint16_to_int32, gather_2_cast_uint16_to_int32, var_23, var_22))[name = string("concat_1")];
            tensor<fp16, [?, ?, 12, 32]> x_3_cast_fp16 = reshape(shape = concat_1, x = dequantize_123)[name = string("x_3_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_value_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12283200))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12430720))))[name = string("model_encoder_layer_0_attention_self_value_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_value_bias_to_fp16 = const()[name = string("model_encoder_layer_0_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12431552)))];
            tensor<fp16, [?, ?, 384]> linear_2_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_value_bias_to_fp16, weight = model_encoder_layer_0_attention_self_value_weight_to_fp16_quantized, x = dequantize_4)[name = string("linear_2_cast_fp16")];
            fp16 quantize_80_scale_0 = const()[name = string("quantize_80_scale_0"), val = fp16(0x1.af4p-6)];
            string quantize_80_output_dtype_0 = const()[name = string("quantize_80_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_124 = quantize(input = linear_2_cast_fp16, output_dtype = quantize_80_output_dtype_0, scale = quantize_80_scale_0)[name = string("quantize_124")];
            fp16 dequantize_80_scale_0 = const()[name = string("dequantize_80_scale_0"), val = fp16(0x1.af4p-6)];
            tensor<fp16, [?, ?, 384]> dequantize_124 = dequantize(input = quantize_124, scale = dequantize_80_scale_0)[name = string("dequantize_124")];
            tensor<int32, [3]> var_109_shape_cast_fp16 = shape(x = dequantize_124)[name = string("op_109_shape_cast_fp16")];
            int32 gather_3_axis_0 = const()[name = string("gather_3_axis_0"), val = int32(0)];
            int32 gather_3_batch_dims_0 = const()[name = string("gather_3_batch_dims_0"), val = int32(0)];
            bool gather_3_validate_indices_0 = const()[name = string("gather_3_validate_indices_0"), val = bool(false)];
            string var_109_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_109_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_3_indices_0_to_uint16 = const()[name = string("gather_3_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_109_shape_cast_fp16_to_uint16 = cast(dtype = var_109_shape_cast_fp16_to_uint16_dtype_0, x = var_109_shape_cast_fp16)[name = string("cast_68")];
            uint16 gather_3_cast_uint16 = gather(axis = gather_3_axis_0, batch_dims = gather_3_batch_dims_0, indices = gather_3_indices_0_to_uint16, validate_indices = gather_3_validate_indices_0, x = var_109_shape_cast_fp16_to_uint16)[name = string("gather_3_cast_uint16")];
            string gather_3_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_3_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_4_axis_0 = const()[name = string("gather_4_axis_0"), val = int32(0)];
            int32 gather_4_batch_dims_0 = const()[name = string("gather_4_batch_dims_0"), val = int32(0)];
            bool gather_4_validate_indices_0 = const()[name = string("gather_4_validate_indices_0"), val = bool(false)];
            uint16 gather_4_indices_0_to_uint16 = const()[name = string("gather_4_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_4_cast_uint16 = gather(axis = gather_4_axis_0, batch_dims = gather_4_batch_dims_0, indices = gather_4_indices_0_to_uint16, validate_indices = gather_4_validate_indices_0, x = var_109_shape_cast_fp16_to_uint16)[name = string("gather_4_cast_uint16")];
            string gather_4_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_4_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_2_axis_0 = const()[name = string("concat_2_axis_0"), val = int32(0)];
            bool concat_2_interleave_0 = const()[name = string("concat_2_interleave_0"), val = bool(false)];
            int32 gather_4_cast_uint16_to_int32 = cast(dtype = gather_4_cast_uint16_to_int32_dtype_0, x = gather_4_cast_uint16)[name = string("cast_66")];
            int32 gather_3_cast_uint16_to_int32 = cast(dtype = gather_3_cast_uint16_to_int32_dtype_0, x = gather_3_cast_uint16)[name = string("cast_67")];
            tensor<int32, [4]> concat_2 = concat(axis = concat_2_axis_0, interleave = concat_2_interleave_0, values = (gather_3_cast_uint16_to_int32, gather_4_cast_uint16_to_int32, var_23, var_22))[name = string("concat_2")];
            tensor<fp16, [?, ?, 12, 32]> x_7_cast_fp16 = reshape(shape = concat_2, x = dequantize_124)[name = string("x_7_cast_fp16")];
            tensor<int32, [4]> var_113 = const()[name = string("op_113"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [3]> var_115_shape_cast_fp16 = shape(x = dequantize_122)[name = string("op_115_shape_cast_fp16")];
            int32 gather_5_axis_0 = const()[name = string("gather_5_axis_0"), val = int32(0)];
            int32 gather_5_batch_dims_0 = const()[name = string("gather_5_batch_dims_0"), val = int32(0)];
            bool gather_5_validate_indices_0 = const()[name = string("gather_5_validate_indices_0"), val = bool(false)];
            string var_115_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_115_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_5_indices_0_to_uint16 = const()[name = string("gather_5_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_115_shape_cast_fp16_to_uint16 = cast(dtype = var_115_shape_cast_fp16_to_uint16_dtype_0, x = var_115_shape_cast_fp16)[name = string("cast_65")];
            uint16 gather_5_cast_uint16 = gather(axis = gather_5_axis_0, batch_dims = gather_5_batch_dims_0, indices = gather_5_indices_0_to_uint16, validate_indices = gather_5_validate_indices_0, x = var_115_shape_cast_fp16_to_uint16)[name = string("gather_5_cast_uint16")];
            string gather_5_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_5_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_6_axis_0 = const()[name = string("gather_6_axis_0"), val = int32(0)];
            int32 gather_6_batch_dims_0 = const()[name = string("gather_6_batch_dims_0"), val = int32(0)];
            bool gather_6_validate_indices_0 = const()[name = string("gather_6_validate_indices_0"), val = bool(false)];
            uint16 gather_6_indices_0_to_uint16 = const()[name = string("gather_6_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_6_cast_uint16 = gather(axis = gather_6_axis_0, batch_dims = gather_6_batch_dims_0, indices = gather_6_indices_0_to_uint16, validate_indices = gather_6_validate_indices_0, x = var_115_shape_cast_fp16_to_uint16)[name = string("gather_6_cast_uint16")];
            string gather_6_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_6_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_3_axis_0 = const()[name = string("concat_3_axis_0"), val = int32(0)];
            bool concat_3_interleave_0 = const()[name = string("concat_3_interleave_0"), val = bool(false)];
            int32 gather_6_cast_uint16_to_int32 = cast(dtype = gather_6_cast_uint16_to_int32_dtype_0, x = gather_6_cast_uint16)[name = string("cast_63")];
            int32 gather_5_cast_uint16_to_int32 = cast(dtype = gather_5_cast_uint16_to_int32_dtype_0, x = gather_5_cast_uint16)[name = string("cast_64")];
            tensor<int32, [4]> concat_3 = concat(axis = concat_3_axis_0, interleave = concat_3_interleave_0, values = (gather_5_cast_uint16_to_int32, gather_6_cast_uint16_to_int32, var_23, var_22))[name = string("concat_3")];
            tensor<fp16, [?, ?, 12, 32]> x_11_cast_fp16 = reshape(shape = concat_3, x = dequantize_122)[name = string("x_11_cast_fp16")];
            bool attention_scores_1_transpose_x_0 = const()[name = string("attention_scores_1_transpose_x_0"), val = bool(false)];
            bool attention_scores_1_transpose_y_0 = const()[name = string("attention_scores_1_transpose_y_0"), val = bool(false)];
            tensor<int32, [4]> transpose_24_perm_0 = const()[name = string("transpose_24_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_25_perm_0 = const()[name = string("transpose_25_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [?, 12, 32, ?]> transpose_25 = transpose(perm = transpose_25_perm_0, x = x_3_cast_fp16)[name = string("transpose_58")];
            tensor<fp16, [?, 12, ?, 32]> transpose_24 = transpose(perm = transpose_24_perm_0, x = x_11_cast_fp16)[name = string("transpose_59")];
            tensor<fp16, [?, 12, ?, ?]> attention_scores_1_cast_fp16 = matmul(transpose_x = attention_scores_1_transpose_x_0, transpose_y = attention_scores_1_transpose_y_0, x = transpose_24, y = transpose_25)[name = string("attention_scores_1_cast_fp16")];
            fp16 _inversed_attention_scores_3_y_0_to_fp16 = const()[name = string("_inversed_attention_scores_3_y_0_to_fp16"), val = fp16(0x1.6ap-3)];
            tensor<fp16, [?, 12, ?, ?]> _inversed_attention_scores_3_cast_fp16 = mul(x = attention_scores_1_cast_fp16, y = _inversed_attention_scores_3_y_0_to_fp16)[name = string("_inversed_attention_scores_3_cast_fp16")];
            fp16 quantize_7_scale_0 = const()[name = string("quantize_7_scale_0"), val = fp16(0x1.2ecp+0)];
            string quantize_7_output_dtype_0 = const()[name = string("quantize_7_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 12, ?, ?]> quantize_7 = quantize(input = _inversed_attention_scores_3_cast_fp16, output_dtype = quantize_7_output_dtype_0, scale = quantize_7_scale_0)[name = string("quantize_7")];
            fp16 quantize_8_scale_0 = const()[name = string("quantize_8_scale_0"), val = fp16(nan)];
            string quantize_8_output_dtype_0 = const()[name = string("quantize_8_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 1, 1, ?]> quantize_8 = quantize(input = attention_mask_cast_fp16, output_dtype = quantize_8_output_dtype_0, scale = quantize_8_scale_0)[name = string("quantize_8")];
            fp16 dequantize_170_scale_0 = const()[name = string("dequantize_170_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, 1, 1, ?]> dequantize_170 = dequantize(input = quantize_8, scale = dequantize_170_scale_0)[name = string("dequantize_170")];
            fp16 dequantize_4_scale_0_1 = const()[name = string("dequantize_4_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_4_1 = dequantize(input = quantize_7, scale = dequantize_4_scale_0_1)[name = string("dequantize_4_1")];
            tensor<fp16, [?, 12, ?, ?]> input_11_cast_fp16 = add(x = dequantize_4_1, y = dequantize_170)[name = string("input_11_cast_fp16")];
            string quantize_81_output_dtype_0 = const()[name = string("quantize_81_output_dtype_0"), val = string("int8")];
            fp16 quantize_2_scale_0 = const()[name = string("quantize_2_scale_0"), val = fp16(nan)];
            tensor<int8, [?, 12, ?, ?]> quantize_2 = quantize(input = input_11_cast_fp16, output_dtype = quantize_81_output_dtype_0, scale = quantize_2_scale_0)[name = string("quantize_2")];
            fp16 dequantize_81_scale_0 = const()[name = string("dequantize_81_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_125 = dequantize(input = quantize_2, scale = dequantize_81_scale_0)[name = string("dequantize_125")];
            tensor<fp16, [?, 12, ?, ?]> input_13_cast_fp16 = softmax(axis = var_24, x = dequantize_125)[name = string("input_13_cast_fp16")];
            bool context_layer_1_transpose_x_0 = const()[name = string("context_layer_1_transpose_x_0"), val = bool(false)];
            bool context_layer_1_transpose_y_0 = const()[name = string("context_layer_1_transpose_y_0"), val = bool(false)];
            tensor<fp16, [?, 12, ?, 32]> value_layer_1_cast_fp16 = transpose(perm = var_113, x = x_7_cast_fp16)[name = string("transpose_57")];
            tensor<fp16, [?, 12, ?, 32]> context_layer_1_cast_fp16 = matmul(transpose_x = context_layer_1_transpose_x_0, transpose_y = context_layer_1_transpose_y_0, x = input_13_cast_fp16, y = value_layer_1_cast_fp16)[name = string("context_layer_1_cast_fp16")];
            tensor<int32, [4]> var_129 = const()[name = string("op_129"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [?, ?, 12, 32]> var_130_cast_fp16 = transpose(perm = var_129, x = context_layer_1_cast_fp16)[name = string("transpose_56")];
            tensor<int32, [4]> var_132_shape_cast_fp16 = shape(x = var_130_cast_fp16)[name = string("op_132_shape_cast_fp16")];
            int32 gather_7_axis_0 = const()[name = string("gather_7_axis_0"), val = int32(0)];
            int32 gather_7_batch_dims_0 = const()[name = string("gather_7_batch_dims_0"), val = int32(0)];
            bool gather_7_validate_indices_0 = const()[name = string("gather_7_validate_indices_0"), val = bool(false)];
            string var_132_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_132_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_7_indices_0_to_uint16 = const()[name = string("gather_7_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [4]> var_132_shape_cast_fp16_to_uint16 = cast(dtype = var_132_shape_cast_fp16_to_uint16_dtype_0, x = var_132_shape_cast_fp16)[name = string("cast_62")];
            uint16 gather_7_cast_uint16 = gather(axis = gather_7_axis_0, batch_dims = gather_7_batch_dims_0, indices = gather_7_indices_0_to_uint16, validate_indices = gather_7_validate_indices_0, x = var_132_shape_cast_fp16_to_uint16)[name = string("gather_7_cast_uint16")];
            string gather_7_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_7_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_8_axis_0 = const()[name = string("gather_8_axis_0"), val = int32(0)];
            int32 gather_8_batch_dims_0 = const()[name = string("gather_8_batch_dims_0"), val = int32(0)];
            bool gather_8_validate_indices_0 = const()[name = string("gather_8_validate_indices_0"), val = bool(false)];
            uint16 gather_8_indices_0_to_uint16 = const()[name = string("gather_8_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_8_cast_uint16 = gather(axis = gather_8_axis_0, batch_dims = gather_8_batch_dims_0, indices = gather_8_indices_0_to_uint16, validate_indices = gather_8_validate_indices_0, x = var_132_shape_cast_fp16_to_uint16)[name = string("gather_8_cast_uint16")];
            string gather_8_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_8_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_4_axis_0 = const()[name = string("concat_4_axis_0"), val = int32(0)];
            bool concat_4_interleave_0 = const()[name = string("concat_4_interleave_0"), val = bool(false)];
            int32 gather_8_cast_uint16_to_int32 = cast(dtype = gather_8_cast_uint16_to_int32_dtype_0, x = gather_8_cast_uint16)[name = string("cast_60")];
            int32 gather_7_cast_uint16_to_int32 = cast(dtype = gather_7_cast_uint16_to_int32_dtype_0, x = gather_7_cast_uint16)[name = string("cast_61")];
            tensor<int32, [3]> concat_4 = concat(axis = concat_4_axis_0, interleave = concat_4_interleave_0, values = (gather_7_cast_uint16_to_int32, gather_8_cast_uint16_to_int32, var_27))[name = string("concat_4")];
            tensor<fp16, [?, ?, 384]> input_15_cast_fp16 = reshape(shape = concat_4, x = var_130_cast_fp16)[name = string("input_15_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12432384))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12579904))))[name = string("model_encoder_layer_0_attention_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_0_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12580736)))];
            fp16 quantize_9_scale_0 = const()[name = string("quantize_9_scale_0"), val = fp16(nan)];
            string quantize_9_output_dtype_0 = const()[name = string("quantize_9_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_9 = quantize(input = input_15_cast_fp16, output_dtype = quantize_9_output_dtype_0, scale = quantize_9_scale_0)[name = string("quantize_9")];
            fp16 dequantize_9_scale_0 = const()[name = string("dequantize_9_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_9 = dequantize(input = quantize_9, scale = dequantize_9_scale_0)[name = string("dequantize_9")];
            tensor<fp16, [?, ?, 384]> linear_3_cast_fp16 = linear(bias = model_encoder_layer_0_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_0_attention_output_dense_weight_to_fp16_quantized, x = dequantize_9)[name = string("linear_3_cast_fp16")];
            fp16 quantize_10_scale_0 = const()[name = string("quantize_10_scale_0"), val = fp16(nan)];
            string quantize_10_output_dtype_0 = const()[name = string("quantize_10_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_10 = quantize(input = linear_3_cast_fp16, output_dtype = quantize_10_output_dtype_0, scale = quantize_10_scale_0)[name = string("quantize_10")];
            fp16 dequantize_172_scale_0 = const()[name = string("dequantize_172_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_172 = dequantize(input = quantize_4, scale = dequantize_172_scale_0)[name = string("dequantize_172")];
            fp16 dequantize_6_scale_0 = const()[name = string("dequantize_6_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_6 = dequantize(input = quantize_10, scale = dequantize_6_scale_0)[name = string("dequantize_6")];
            tensor<fp16, [?, ?, 384]> input_19_cast_fp16 = add(x = dequantize_6, y = dequantize_172)[name = string("input_19_cast_fp16")];
            string quantize_82_output_dtype_0 = const()[name = string("quantize_82_output_dtype_0"), val = string("int8")];
            fp16 quantize_3_scale_0_1 = const()[name = string("quantize_3_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_3_1 = quantize(input = input_19_cast_fp16, output_dtype = quantize_82_output_dtype_0, scale = quantize_3_scale_0_1)[name = string("quantize_3_1")];
            fp16 dequantize_82_scale_0 = const()[name = string("dequantize_82_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_126 = dequantize(input = quantize_3_1, scale = dequantize_82_scale_0)[name = string("dequantize_126")];
            tensor<int32, [1]> input_21_axes_0 = const()[name = string("input_21_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12581568)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12582400)))];
            tensor<fp16, [?, ?, 384]> input_21_cast_fp16 = layer_norm(axes = input_21_axes_0, beta = model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16, x = dequantize_126)[name = string("input_21_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_0_intermediate_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [1536, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(12583232))), scale = tensor<fp16, [1536, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13173120))))[name = string("model_encoder_layer_0_intermediate_dense_weight_to_fp16_quantized")];
            tensor<fp16, [1536]> model_encoder_layer_0_intermediate_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_0_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13176256)))];
            fp16 quantize_12_scale_0 = const()[name = string("quantize_12_scale_0"), val = fp16(nan)];
            string quantize_12_output_dtype_0 = const()[name = string("quantize_12_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_12 = quantize(input = input_21_cast_fp16, output_dtype = quantize_12_output_dtype_0, scale = quantize_12_scale_0)[name = string("quantize_12")];
            fp16 dequantize_12_scale_0 = const()[name = string("dequantize_12_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_12 = dequantize(input = quantize_12, scale = dequantize_12_scale_0)[name = string("dequantize_12")];
            tensor<fp16, [?, ?, 1536]> linear_4_cast_fp16 = linear(bias = model_encoder_layer_0_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_0_intermediate_dense_weight_to_fp16_quantized, x = dequantize_12)[name = string("linear_4_cast_fp16")];
            fp16 quantize_83_scale_0 = const()[name = string("quantize_83_scale_0"), val = fp16(nan)];
            string quantize_83_output_dtype_0 = const()[name = string("quantize_83_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_127 = quantize(input = linear_4_cast_fp16, output_dtype = quantize_83_output_dtype_0, scale = quantize_83_scale_0)[name = string("quantize_127")];
            fp16 dequantize_83_scale_0 = const()[name = string("dequantize_83_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_127 = dequantize(input = quantize_127, scale = dequantize_83_scale_0)[name = string("dequantize_127")];
            string input_25_mode_0 = const()[name = string("input_25_mode_0"), val = string("EXACT")];
            tensor<fp16, [?, ?, 1536]> input_25_cast_fp16 = gelu(mode = input_25_mode_0, x = dequantize_127)[name = string("input_25_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_0_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13179392))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13769280))))[name = string("model_encoder_layer_0_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_0_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_0_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13770112)))];
            fp16 quantize_13_scale_0 = const()[name = string("quantize_13_scale_0"), val = fp16(nan)];
            string quantize_13_output_dtype_0 = const()[name = string("quantize_13_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_13 = quantize(input = input_25_cast_fp16, output_dtype = quantize_13_output_dtype_0, scale = quantize_13_scale_0)[name = string("quantize_13")];
            fp16 dequantize_13_scale_0 = const()[name = string("dequantize_13_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_13 = dequantize(input = quantize_13, scale = dequantize_13_scale_0)[name = string("dequantize_13")];
            tensor<fp16, [?, ?, 384]> linear_5_cast_fp16 = linear(bias = model_encoder_layer_0_output_dense_bias_to_fp16, weight = model_encoder_layer_0_output_dense_weight_to_fp16_quantized, x = dequantize_13)[name = string("linear_5_cast_fp16")];
            fp16 quantize_14_scale_0 = const()[name = string("quantize_14_scale_0"), val = fp16(nan)];
            string quantize_14_output_dtype_0 = const()[name = string("quantize_14_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_14 = quantize(input = linear_5_cast_fp16, output_dtype = quantize_14_output_dtype_0, scale = quantize_14_scale_0)[name = string("quantize_14")];
            fp16 quantize_15_scale_0 = const()[name = string("quantize_15_scale_0"), val = fp16(nan)];
            string quantize_15_output_dtype_0 = const()[name = string("quantize_15_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_15 = quantize(input = input_21_cast_fp16, output_dtype = quantize_15_output_dtype_0, scale = quantize_15_scale_0)[name = string("quantize_15")];
            fp16 dequantize_174_scale_0 = const()[name = string("dequantize_174_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_174 = dequantize(input = quantize_15, scale = dequantize_174_scale_0)[name = string("dequantize_174")];
            fp16 dequantize_8_scale_0 = const()[name = string("dequantize_8_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_8 = dequantize(input = quantize_14, scale = dequantize_8_scale_0)[name = string("dequantize_8")];
            tensor<fp16, [?, ?, 384]> input_29_cast_fp16 = add(x = dequantize_8, y = dequantize_174)[name = string("input_29_cast_fp16")];
            string quantize_84_output_dtype_0 = const()[name = string("quantize_84_output_dtype_0"), val = string("int8")];
            fp16 quantize_4_scale_0_1 = const()[name = string("quantize_4_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_4_1 = quantize(input = input_29_cast_fp16, output_dtype = quantize_84_output_dtype_0, scale = quantize_4_scale_0_1)[name = string("quantize_4_1")];
            fp16 dequantize_84_scale_0 = const()[name = string("dequantize_84_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_128 = dequantize(input = quantize_4_1, scale = dequantize_84_scale_0)[name = string("dequantize_128")];
            tensor<int32, [1]> input_31_axes_0 = const()[name = string("input_31_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_0_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13770944)))];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_0_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13771776)))];
            tensor<fp16, [?, ?, 384]> input_31_cast_fp16 = layer_norm(axes = input_31_axes_0, beta = model_encoder_layer_0_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_0_output_LayerNorm_weight_to_fp16, x = dequantize_128)[name = string("input_31_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_query_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13772608))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13920128))))[name = string("model_encoder_layer_1_attention_self_query_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_query_bias_to_fp16 = const()[name = string("model_encoder_layer_1_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13920960)))];
            fp16 quantize_16_scale_0 = const()[name = string("quantize_16_scale_0"), val = fp16(nan)];
            string quantize_16_output_dtype_0 = const()[name = string("quantize_16_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_16 = quantize(input = input_31_cast_fp16, output_dtype = quantize_16_output_dtype_0, scale = quantize_16_scale_0)[name = string("quantize_16")];
            fp16 dequantize_16_scale_0 = const()[name = string("dequantize_16_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_16 = dequantize(input = quantize_16, scale = dequantize_16_scale_0)[name = string("dequantize_16")];
            tensor<fp16, [?, ?, 384]> linear_6_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_query_bias_to_fp16, weight = model_encoder_layer_1_attention_self_query_weight_to_fp16_quantized, x = dequantize_16)[name = string("linear_6_cast_fp16")];
            fp16 quantize_85_scale_0 = const()[name = string("quantize_85_scale_0"), val = fp16(nan)];
            string quantize_85_output_dtype_0 = const()[name = string("quantize_85_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_129 = quantize(input = linear_6_cast_fp16, output_dtype = quantize_85_output_dtype_0, scale = quantize_85_scale_0)[name = string("quantize_129")];
            fp16 dequantize_85_scale_0 = const()[name = string("dequantize_85_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_129 = dequantize(input = quantize_129, scale = dequantize_85_scale_0)[name = string("dequantize_129")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_key_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(13921792))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14069312))))[name = string("model_encoder_layer_1_attention_self_key_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_key_bias_to_fp16 = const()[name = string("model_encoder_layer_1_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14070144)))];
            fp16 quantize_17_scale_0 = const()[name = string("quantize_17_scale_0"), val = fp16(nan)];
            string quantize_17_output_dtype_0 = const()[name = string("quantize_17_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_17 = quantize(input = input_31_cast_fp16, output_dtype = quantize_17_output_dtype_0, scale = quantize_17_scale_0)[name = string("quantize_17")];
            fp16 dequantize_17_scale_0 = const()[name = string("dequantize_17_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_17 = dequantize(input = quantize_17, scale = dequantize_17_scale_0)[name = string("dequantize_17")];
            tensor<fp16, [?, ?, 384]> linear_7_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_key_bias_to_fp16, weight = model_encoder_layer_1_attention_self_key_weight_to_fp16_quantized, x = dequantize_17)[name = string("linear_7_cast_fp16")];
            fp16 quantize_86_scale_0 = const()[name = string("quantize_86_scale_0"), val = fp16(nan)];
            string quantize_86_output_dtype_0 = const()[name = string("quantize_86_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_130 = quantize(input = linear_7_cast_fp16, output_dtype = quantize_86_output_dtype_0, scale = quantize_86_scale_0)[name = string("quantize_130")];
            fp16 dequantize_86_scale_0 = const()[name = string("dequantize_86_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_130 = dequantize(input = quantize_130, scale = dequantize_86_scale_0)[name = string("dequantize_130")];
            tensor<int32, [3]> var_177_shape_cast_fp16 = shape(x = dequantize_130)[name = string("op_177_shape_cast_fp16")];
            int32 gather_9_axis_0 = const()[name = string("gather_9_axis_0"), val = int32(0)];
            int32 gather_9_batch_dims_0 = const()[name = string("gather_9_batch_dims_0"), val = int32(0)];
            bool gather_9_validate_indices_0 = const()[name = string("gather_9_validate_indices_0"), val = bool(false)];
            string var_177_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_177_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_9_indices_0_to_uint16 = const()[name = string("gather_9_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_177_shape_cast_fp16_to_uint16 = cast(dtype = var_177_shape_cast_fp16_to_uint16_dtype_0, x = var_177_shape_cast_fp16)[name = string("cast_59")];
            uint16 gather_9_cast_uint16 = gather(axis = gather_9_axis_0, batch_dims = gather_9_batch_dims_0, indices = gather_9_indices_0_to_uint16, validate_indices = gather_9_validate_indices_0, x = var_177_shape_cast_fp16_to_uint16)[name = string("gather_9_cast_uint16")];
            string gather_9_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_9_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_10_axis_0 = const()[name = string("gather_10_axis_0"), val = int32(0)];
            int32 gather_10_batch_dims_0 = const()[name = string("gather_10_batch_dims_0"), val = int32(0)];
            bool gather_10_validate_indices_0 = const()[name = string("gather_10_validate_indices_0"), val = bool(false)];
            uint16 gather_10_indices_0_to_uint16 = const()[name = string("gather_10_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_10_cast_uint16 = gather(axis = gather_10_axis_0, batch_dims = gather_10_batch_dims_0, indices = gather_10_indices_0_to_uint16, validate_indices = gather_10_validate_indices_0, x = var_177_shape_cast_fp16_to_uint16)[name = string("gather_10_cast_uint16")];
            string gather_10_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_10_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_5_axis_0 = const()[name = string("concat_5_axis_0"), val = int32(0)];
            bool concat_5_interleave_0 = const()[name = string("concat_5_interleave_0"), val = bool(false)];
            int32 gather_10_cast_uint16_to_int32 = cast(dtype = gather_10_cast_uint16_to_int32_dtype_0, x = gather_10_cast_uint16)[name = string("cast_57")];
            int32 gather_9_cast_uint16_to_int32 = cast(dtype = gather_9_cast_uint16_to_int32_dtype_0, x = gather_9_cast_uint16)[name = string("cast_58")];
            tensor<int32, [4]> concat_5 = concat(axis = concat_5_axis_0, interleave = concat_5_interleave_0, values = (gather_9_cast_uint16_to_int32, gather_10_cast_uint16_to_int32, var_23, var_22))[name = string("concat_5")];
            tensor<fp16, [?, ?, 12, 32]> x_15_cast_fp16 = reshape(shape = concat_5, x = dequantize_130)[name = string("x_15_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_value_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14070976))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14218496))))[name = string("model_encoder_layer_1_attention_self_value_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_value_bias_to_fp16 = const()[name = string("model_encoder_layer_1_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14219328)))];
            fp16 quantize_18_scale_0 = const()[name = string("quantize_18_scale_0"), val = fp16(nan)];
            string quantize_18_output_dtype_0 = const()[name = string("quantize_18_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_18 = quantize(input = input_31_cast_fp16, output_dtype = quantize_18_output_dtype_0, scale = quantize_18_scale_0)[name = string("quantize_18")];
            fp16 dequantize_18_scale_0 = const()[name = string("dequantize_18_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_18 = dequantize(input = quantize_18, scale = dequantize_18_scale_0)[name = string("dequantize_18")];
            tensor<fp16, [?, ?, 384]> linear_8_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_value_bias_to_fp16, weight = model_encoder_layer_1_attention_self_value_weight_to_fp16_quantized, x = dequantize_18)[name = string("linear_8_cast_fp16")];
            fp16 quantize_87_scale_0 = const()[name = string("quantize_87_scale_0"), val = fp16(nan)];
            string quantize_87_output_dtype_0 = const()[name = string("quantize_87_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_131 = quantize(input = linear_8_cast_fp16, output_dtype = quantize_87_output_dtype_0, scale = quantize_87_scale_0)[name = string("quantize_131")];
            fp16 dequantize_87_scale_0 = const()[name = string("dequantize_87_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_131 = dequantize(input = quantize_131, scale = dequantize_87_scale_0)[name = string("dequantize_131")];
            tensor<int32, [3]> var_186_shape_cast_fp16 = shape(x = dequantize_131)[name = string("op_186_shape_cast_fp16")];
            int32 gather_11_axis_0 = const()[name = string("gather_11_axis_0"), val = int32(0)];
            int32 gather_11_batch_dims_0 = const()[name = string("gather_11_batch_dims_0"), val = int32(0)];
            bool gather_11_validate_indices_0 = const()[name = string("gather_11_validate_indices_0"), val = bool(false)];
            string var_186_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_186_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_11_indices_0_to_uint16 = const()[name = string("gather_11_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_186_shape_cast_fp16_to_uint16 = cast(dtype = var_186_shape_cast_fp16_to_uint16_dtype_0, x = var_186_shape_cast_fp16)[name = string("cast_56")];
            uint16 gather_11_cast_uint16 = gather(axis = gather_11_axis_0, batch_dims = gather_11_batch_dims_0, indices = gather_11_indices_0_to_uint16, validate_indices = gather_11_validate_indices_0, x = var_186_shape_cast_fp16_to_uint16)[name = string("gather_11_cast_uint16")];
            string gather_11_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_11_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_12_axis_0 = const()[name = string("gather_12_axis_0"), val = int32(0)];
            int32 gather_12_batch_dims_0 = const()[name = string("gather_12_batch_dims_0"), val = int32(0)];
            bool gather_12_validate_indices_0 = const()[name = string("gather_12_validate_indices_0"), val = bool(false)];
            uint16 gather_12_indices_0_to_uint16 = const()[name = string("gather_12_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_12_cast_uint16 = gather(axis = gather_12_axis_0, batch_dims = gather_12_batch_dims_0, indices = gather_12_indices_0_to_uint16, validate_indices = gather_12_validate_indices_0, x = var_186_shape_cast_fp16_to_uint16)[name = string("gather_12_cast_uint16")];
            string gather_12_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_12_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_6_axis_0 = const()[name = string("concat_6_axis_0"), val = int32(0)];
            bool concat_6_interleave_0 = const()[name = string("concat_6_interleave_0"), val = bool(false)];
            int32 gather_12_cast_uint16_to_int32 = cast(dtype = gather_12_cast_uint16_to_int32_dtype_0, x = gather_12_cast_uint16)[name = string("cast_54")];
            int32 gather_11_cast_uint16_to_int32 = cast(dtype = gather_11_cast_uint16_to_int32_dtype_0, x = gather_11_cast_uint16)[name = string("cast_55")];
            tensor<int32, [4]> concat_6 = concat(axis = concat_6_axis_0, interleave = concat_6_interleave_0, values = (gather_11_cast_uint16_to_int32, gather_12_cast_uint16_to_int32, var_23, var_22))[name = string("concat_6")];
            tensor<fp16, [?, ?, 12, 32]> x_19_cast_fp16 = reshape(shape = concat_6, x = dequantize_131)[name = string("x_19_cast_fp16")];
            tensor<int32, [4]> var_190 = const()[name = string("op_190"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [3]> var_192_shape_cast_fp16 = shape(x = dequantize_129)[name = string("op_192_shape_cast_fp16")];
            int32 gather_13_axis_0 = const()[name = string("gather_13_axis_0"), val = int32(0)];
            int32 gather_13_batch_dims_0 = const()[name = string("gather_13_batch_dims_0"), val = int32(0)];
            bool gather_13_validate_indices_0 = const()[name = string("gather_13_validate_indices_0"), val = bool(false)];
            string var_192_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_192_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_13_indices_0_to_uint16 = const()[name = string("gather_13_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_192_shape_cast_fp16_to_uint16 = cast(dtype = var_192_shape_cast_fp16_to_uint16_dtype_0, x = var_192_shape_cast_fp16)[name = string("cast_53")];
            uint16 gather_13_cast_uint16 = gather(axis = gather_13_axis_0, batch_dims = gather_13_batch_dims_0, indices = gather_13_indices_0_to_uint16, validate_indices = gather_13_validate_indices_0, x = var_192_shape_cast_fp16_to_uint16)[name = string("gather_13_cast_uint16")];
            string gather_13_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_13_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_14_axis_0 = const()[name = string("gather_14_axis_0"), val = int32(0)];
            int32 gather_14_batch_dims_0 = const()[name = string("gather_14_batch_dims_0"), val = int32(0)];
            bool gather_14_validate_indices_0 = const()[name = string("gather_14_validate_indices_0"), val = bool(false)];
            uint16 gather_14_indices_0_to_uint16 = const()[name = string("gather_14_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_14_cast_uint16 = gather(axis = gather_14_axis_0, batch_dims = gather_14_batch_dims_0, indices = gather_14_indices_0_to_uint16, validate_indices = gather_14_validate_indices_0, x = var_192_shape_cast_fp16_to_uint16)[name = string("gather_14_cast_uint16")];
            string gather_14_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_14_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_7_axis_0 = const()[name = string("concat_7_axis_0"), val = int32(0)];
            bool concat_7_interleave_0 = const()[name = string("concat_7_interleave_0"), val = bool(false)];
            int32 gather_14_cast_uint16_to_int32 = cast(dtype = gather_14_cast_uint16_to_int32_dtype_0, x = gather_14_cast_uint16)[name = string("cast_51")];
            int32 gather_13_cast_uint16_to_int32 = cast(dtype = gather_13_cast_uint16_to_int32_dtype_0, x = gather_13_cast_uint16)[name = string("cast_52")];
            tensor<int32, [4]> concat_7 = concat(axis = concat_7_axis_0, interleave = concat_7_interleave_0, values = (gather_13_cast_uint16_to_int32, gather_14_cast_uint16_to_int32, var_23, var_22))[name = string("concat_7")];
            tensor<fp16, [?, ?, 12, 32]> x_23_cast_fp16 = reshape(shape = concat_7, x = dequantize_129)[name = string("x_23_cast_fp16")];
            bool attention_scores_5_transpose_x_0 = const()[name = string("attention_scores_5_transpose_x_0"), val = bool(false)];
            bool attention_scores_5_transpose_y_0 = const()[name = string("attention_scores_5_transpose_y_0"), val = bool(false)];
            tensor<int32, [4]> transpose_26_perm_0 = const()[name = string("transpose_26_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_27_perm_0 = const()[name = string("transpose_27_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [?, 12, 32, ?]> transpose_27 = transpose(perm = transpose_27_perm_0, x = x_15_cast_fp16)[name = string("transpose_54")];
            tensor<fp16, [?, 12, ?, 32]> transpose_26 = transpose(perm = transpose_26_perm_0, x = x_23_cast_fp16)[name = string("transpose_55")];
            tensor<fp16, [?, 12, ?, ?]> attention_scores_5_cast_fp16 = matmul(transpose_x = attention_scores_5_transpose_x_0, transpose_y = attention_scores_5_transpose_y_0, x = transpose_26, y = transpose_27)[name = string("attention_scores_5_cast_fp16")];
            fp16 _inversed_attention_scores_7_y_0_to_fp16 = const()[name = string("_inversed_attention_scores_7_y_0_to_fp16"), val = fp16(0x1.6ap-3)];
            tensor<fp16, [?, 12, ?, ?]> _inversed_attention_scores_7_cast_fp16 = mul(x = attention_scores_5_cast_fp16, y = _inversed_attention_scores_7_y_0_to_fp16)[name = string("_inversed_attention_scores_7_cast_fp16")];
            fp16 quantize_19_scale_0 = const()[name = string("quantize_19_scale_0"), val = fp16(nan)];
            string quantize_19_output_dtype_0 = const()[name = string("quantize_19_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 12, ?, ?]> quantize_19 = quantize(input = _inversed_attention_scores_7_cast_fp16, output_dtype = quantize_19_output_dtype_0, scale = quantize_19_scale_0)[name = string("quantize_19")];
            fp16 quantize_20_scale_0 = const()[name = string("quantize_20_scale_0"), val = fp16(nan)];
            string quantize_20_output_dtype_0 = const()[name = string("quantize_20_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 1, 1, ?]> quantize_20 = quantize(input = attention_mask_cast_fp16, output_dtype = quantize_20_output_dtype_0, scale = quantize_20_scale_0)[name = string("quantize_20")];
            fp16 dequantize_176_scale_0 = const()[name = string("dequantize_176_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, 1, 1, ?]> dequantize_176 = dequantize(input = quantize_20, scale = dequantize_176_scale_0)[name = string("dequantize_176")];
            fp16 dequantize_10_scale_0 = const()[name = string("dequantize_10_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_10 = dequantize(input = quantize_19, scale = dequantize_10_scale_0)[name = string("dequantize_10")];
            tensor<fp16, [?, 12, ?, ?]> input_33_cast_fp16 = add(x = dequantize_10, y = dequantize_176)[name = string("input_33_cast_fp16")];
            string quantize_88_output_dtype_0 = const()[name = string("quantize_88_output_dtype_0"), val = string("int8")];
            fp16 quantize_5_scale_0 = const()[name = string("quantize_5_scale_0"), val = fp16(nan)];
            tensor<int8, [?, 12, ?, ?]> quantize_5 = quantize(input = input_33_cast_fp16, output_dtype = quantize_88_output_dtype_0, scale = quantize_5_scale_0)[name = string("quantize_5")];
            fp16 dequantize_88_scale_0 = const()[name = string("dequantize_88_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_132 = dequantize(input = quantize_5, scale = dequantize_88_scale_0)[name = string("dequantize_132")];
            tensor<fp16, [?, 12, ?, ?]> input_35_cast_fp16 = softmax(axis = var_24, x = dequantize_132)[name = string("input_35_cast_fp16")];
            bool context_layer_5_transpose_x_0 = const()[name = string("context_layer_5_transpose_x_0"), val = bool(false)];
            bool context_layer_5_transpose_y_0 = const()[name = string("context_layer_5_transpose_y_0"), val = bool(false)];
            tensor<fp16, [?, 12, ?, 32]> value_layer_3_cast_fp16 = transpose(perm = var_190, x = x_19_cast_fp16)[name = string("transpose_53")];
            tensor<fp16, [?, 12, ?, 32]> context_layer_5_cast_fp16 = matmul(transpose_x = context_layer_5_transpose_x_0, transpose_y = context_layer_5_transpose_y_0, x = input_35_cast_fp16, y = value_layer_3_cast_fp16)[name = string("context_layer_5_cast_fp16")];
            tensor<int32, [4]> var_206 = const()[name = string("op_206"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [?, ?, 12, 32]> var_207_cast_fp16 = transpose(perm = var_206, x = context_layer_5_cast_fp16)[name = string("transpose_52")];
            tensor<int32, [4]> var_209_shape_cast_fp16 = shape(x = var_207_cast_fp16)[name = string("op_209_shape_cast_fp16")];
            int32 gather_15_axis_0 = const()[name = string("gather_15_axis_0"), val = int32(0)];
            int32 gather_15_batch_dims_0 = const()[name = string("gather_15_batch_dims_0"), val = int32(0)];
            bool gather_15_validate_indices_0 = const()[name = string("gather_15_validate_indices_0"), val = bool(false)];
            string var_209_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_209_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_15_indices_0_to_uint16 = const()[name = string("gather_15_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [4]> var_209_shape_cast_fp16_to_uint16 = cast(dtype = var_209_shape_cast_fp16_to_uint16_dtype_0, x = var_209_shape_cast_fp16)[name = string("cast_50")];
            uint16 gather_15_cast_uint16 = gather(axis = gather_15_axis_0, batch_dims = gather_15_batch_dims_0, indices = gather_15_indices_0_to_uint16, validate_indices = gather_15_validate_indices_0, x = var_209_shape_cast_fp16_to_uint16)[name = string("gather_15_cast_uint16")];
            string gather_15_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_15_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_16_axis_0 = const()[name = string("gather_16_axis_0"), val = int32(0)];
            int32 gather_16_batch_dims_0 = const()[name = string("gather_16_batch_dims_0"), val = int32(0)];
            bool gather_16_validate_indices_0 = const()[name = string("gather_16_validate_indices_0"), val = bool(false)];
            uint16 gather_16_indices_0_to_uint16 = const()[name = string("gather_16_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_16_cast_uint16 = gather(axis = gather_16_axis_0, batch_dims = gather_16_batch_dims_0, indices = gather_16_indices_0_to_uint16, validate_indices = gather_16_validate_indices_0, x = var_209_shape_cast_fp16_to_uint16)[name = string("gather_16_cast_uint16")];
            string gather_16_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_16_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_8_axis_0 = const()[name = string("concat_8_axis_0"), val = int32(0)];
            bool concat_8_interleave_0 = const()[name = string("concat_8_interleave_0"), val = bool(false)];
            int32 gather_16_cast_uint16_to_int32 = cast(dtype = gather_16_cast_uint16_to_int32_dtype_0, x = gather_16_cast_uint16)[name = string("cast_48")];
            int32 gather_15_cast_uint16_to_int32 = cast(dtype = gather_15_cast_uint16_to_int32_dtype_0, x = gather_15_cast_uint16)[name = string("cast_49")];
            tensor<int32, [3]> concat_8 = concat(axis = concat_8_axis_0, interleave = concat_8_interleave_0, values = (gather_15_cast_uint16_to_int32, gather_16_cast_uint16_to_int32, var_27))[name = string("concat_8")];
            tensor<fp16, [?, ?, 384]> input_37_cast_fp16 = reshape(shape = concat_8, x = var_207_cast_fp16)[name = string("input_37_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14220160))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14367680))))[name = string("model_encoder_layer_1_attention_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_1_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14368512)))];
            fp16 quantize_21_scale_0 = const()[name = string("quantize_21_scale_0"), val = fp16(nan)];
            string quantize_21_output_dtype_0 = const()[name = string("quantize_21_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_21 = quantize(input = input_37_cast_fp16, output_dtype = quantize_21_output_dtype_0, scale = quantize_21_scale_0)[name = string("quantize_21")];
            fp16 dequantize_21_scale_0 = const()[name = string("dequantize_21_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_21 = dequantize(input = quantize_21, scale = dequantize_21_scale_0)[name = string("dequantize_21")];
            tensor<fp16, [?, ?, 384]> linear_9_cast_fp16 = linear(bias = model_encoder_layer_1_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_1_attention_output_dense_weight_to_fp16_quantized, x = dequantize_21)[name = string("linear_9_cast_fp16")];
            fp16 quantize_22_scale_0 = const()[name = string("quantize_22_scale_0"), val = fp16(nan)];
            string quantize_22_output_dtype_0 = const()[name = string("quantize_22_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_22 = quantize(input = linear_9_cast_fp16, output_dtype = quantize_22_output_dtype_0, scale = quantize_22_scale_0)[name = string("quantize_22")];
            fp16 quantize_23_scale_0 = const()[name = string("quantize_23_scale_0"), val = fp16(nan)];
            string quantize_23_output_dtype_0 = const()[name = string("quantize_23_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_23 = quantize(input = input_31_cast_fp16, output_dtype = quantize_23_output_dtype_0, scale = quantize_23_scale_0)[name = string("quantize_23")];
            fp16 dequantize_178_scale_0 = const()[name = string("dequantize_178_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_178 = dequantize(input = quantize_23, scale = dequantize_178_scale_0)[name = string("dequantize_178")];
            fp16 dequantize_12_scale_0_1 = const()[name = string("dequantize_12_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_12_1 = dequantize(input = quantize_22, scale = dequantize_12_scale_0_1)[name = string("dequantize_12_1")];
            tensor<fp16, [?, ?, 384]> input_41_cast_fp16 = add(x = dequantize_12_1, y = dequantize_178)[name = string("input_41_cast_fp16")];
            string quantize_89_output_dtype_0 = const()[name = string("quantize_89_output_dtype_0"), val = string("int8")];
            fp16 quantize_6_scale_0 = const()[name = string("quantize_6_scale_0"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_6 = quantize(input = input_41_cast_fp16, output_dtype = quantize_89_output_dtype_0, scale = quantize_6_scale_0)[name = string("quantize_6")];
            fp16 dequantize_89_scale_0 = const()[name = string("dequantize_89_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_133 = dequantize(input = quantize_6, scale = dequantize_89_scale_0)[name = string("dequantize_133")];
            tensor<int32, [1]> input_43_axes_0 = const()[name = string("input_43_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14369344)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14370176)))];
            tensor<fp16, [?, ?, 384]> input_43_cast_fp16 = layer_norm(axes = input_43_axes_0, beta = model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16, x = dequantize_133)[name = string("input_43_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_1_intermediate_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [1536, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14371008))), scale = tensor<fp16, [1536, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14960896))))[name = string("model_encoder_layer_1_intermediate_dense_weight_to_fp16_quantized")];
            tensor<fp16, [1536]> model_encoder_layer_1_intermediate_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_1_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14964032)))];
            fp16 quantize_24_scale_0 = const()[name = string("quantize_24_scale_0"), val = fp16(nan)];
            string quantize_24_output_dtype_0 = const()[name = string("quantize_24_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_24 = quantize(input = input_43_cast_fp16, output_dtype = quantize_24_output_dtype_0, scale = quantize_24_scale_0)[name = string("quantize_24")];
            fp16 dequantize_24_scale_0 = const()[name = string("dequantize_24_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_24 = dequantize(input = quantize_24, scale = dequantize_24_scale_0)[name = string("dequantize_24")];
            tensor<fp16, [?, ?, 1536]> linear_10_cast_fp16 = linear(bias = model_encoder_layer_1_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_1_intermediate_dense_weight_to_fp16_quantized, x = dequantize_24)[name = string("linear_10_cast_fp16")];
            fp16 quantize_90_scale_0 = const()[name = string("quantize_90_scale_0"), val = fp16(nan)];
            string quantize_90_output_dtype_0 = const()[name = string("quantize_90_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_134 = quantize(input = linear_10_cast_fp16, output_dtype = quantize_90_output_dtype_0, scale = quantize_90_scale_0)[name = string("quantize_134")];
            fp16 dequantize_90_scale_0 = const()[name = string("dequantize_90_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_134 = dequantize(input = quantize_134, scale = dequantize_90_scale_0)[name = string("dequantize_134")];
            string input_47_mode_0 = const()[name = string("input_47_mode_0"), val = string("EXACT")];
            tensor<fp16, [?, ?, 1536]> input_47_cast_fp16 = gelu(mode = input_47_mode_0, x = dequantize_134)[name = string("input_47_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_1_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(14967168))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15557056))))[name = string("model_encoder_layer_1_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_1_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_1_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15557888)))];
            fp16 quantize_25_scale_0 = const()[name = string("quantize_25_scale_0"), val = fp16(nan)];
            string quantize_25_output_dtype_0 = const()[name = string("quantize_25_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_25 = quantize(input = input_47_cast_fp16, output_dtype = quantize_25_output_dtype_0, scale = quantize_25_scale_0)[name = string("quantize_25")];
            fp16 dequantize_25_scale_0 = const()[name = string("dequantize_25_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_25 = dequantize(input = quantize_25, scale = dequantize_25_scale_0)[name = string("dequantize_25")];
            tensor<fp16, [?, ?, 384]> linear_11_cast_fp16 = linear(bias = model_encoder_layer_1_output_dense_bias_to_fp16, weight = model_encoder_layer_1_output_dense_weight_to_fp16_quantized, x = dequantize_25)[name = string("linear_11_cast_fp16")];
            fp16 quantize_26_scale_0 = const()[name = string("quantize_26_scale_0"), val = fp16(nan)];
            string quantize_26_output_dtype_0 = const()[name = string("quantize_26_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_26 = quantize(input = linear_11_cast_fp16, output_dtype = quantize_26_output_dtype_0, scale = quantize_26_scale_0)[name = string("quantize_26")];
            fp16 quantize_27_scale_0 = const()[name = string("quantize_27_scale_0"), val = fp16(nan)];
            string quantize_27_output_dtype_0 = const()[name = string("quantize_27_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_27 = quantize(input = input_43_cast_fp16, output_dtype = quantize_27_output_dtype_0, scale = quantize_27_scale_0)[name = string("quantize_27")];
            fp16 dequantize_180_scale_0 = const()[name = string("dequantize_180_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_180 = dequantize(input = quantize_27, scale = dequantize_180_scale_0)[name = string("dequantize_180")];
            fp16 dequantize_14_scale_0 = const()[name = string("dequantize_14_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_14 = dequantize(input = quantize_26, scale = dequantize_14_scale_0)[name = string("dequantize_14")];
            tensor<fp16, [?, ?, 384]> input_51_cast_fp16 = add(x = dequantize_14, y = dequantize_180)[name = string("input_51_cast_fp16")];
            string quantize_91_output_dtype_0 = const()[name = string("quantize_91_output_dtype_0"), val = string("int8")];
            fp16 quantize_7_scale_0_1 = const()[name = string("quantize_7_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_7_1 = quantize(input = input_51_cast_fp16, output_dtype = quantize_91_output_dtype_0, scale = quantize_7_scale_0_1)[name = string("quantize_7_1")];
            fp16 dequantize_91_scale_0 = const()[name = string("dequantize_91_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_135 = dequantize(input = quantize_7_1, scale = dequantize_91_scale_0)[name = string("dequantize_135")];
            tensor<int32, [1]> input_53_axes_0 = const()[name = string("input_53_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_1_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15558720)))];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_1_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15559552)))];
            tensor<fp16, [?, ?, 384]> input_53_cast_fp16 = layer_norm(axes = input_53_axes_0, beta = model_encoder_layer_1_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_1_output_LayerNorm_weight_to_fp16, x = dequantize_135)[name = string("input_53_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_query_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15560384))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15707904))))[name = string("model_encoder_layer_2_attention_self_query_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_query_bias_to_fp16 = const()[name = string("model_encoder_layer_2_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15708736)))];
            fp16 quantize_28_scale_0 = const()[name = string("quantize_28_scale_0"), val = fp16(nan)];
            string quantize_28_output_dtype_0 = const()[name = string("quantize_28_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_28 = quantize(input = input_53_cast_fp16, output_dtype = quantize_28_output_dtype_0, scale = quantize_28_scale_0)[name = string("quantize_28")];
            fp16 dequantize_28_scale_0 = const()[name = string("dequantize_28_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_28 = dequantize(input = quantize_28, scale = dequantize_28_scale_0)[name = string("dequantize_28")];
            tensor<fp16, [?, ?, 384]> linear_12_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_query_bias_to_fp16, weight = model_encoder_layer_2_attention_self_query_weight_to_fp16_quantized, x = dequantize_28)[name = string("linear_12_cast_fp16")];
            fp16 quantize_92_scale_0 = const()[name = string("quantize_92_scale_0"), val = fp16(nan)];
            string quantize_92_output_dtype_0 = const()[name = string("quantize_92_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_136 = quantize(input = linear_12_cast_fp16, output_dtype = quantize_92_output_dtype_0, scale = quantize_92_scale_0)[name = string("quantize_136")];
            fp16 dequantize_92_scale_0 = const()[name = string("dequantize_92_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_136 = dequantize(input = quantize_136, scale = dequantize_92_scale_0)[name = string("dequantize_136")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_key_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15709568))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15857088))))[name = string("model_encoder_layer_2_attention_self_key_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_key_bias_to_fp16 = const()[name = string("model_encoder_layer_2_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15857920)))];
            fp16 quantize_29_scale_0 = const()[name = string("quantize_29_scale_0"), val = fp16(nan)];
            string quantize_29_output_dtype_0 = const()[name = string("quantize_29_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_29 = quantize(input = input_53_cast_fp16, output_dtype = quantize_29_output_dtype_0, scale = quantize_29_scale_0)[name = string("quantize_29")];
            fp16 dequantize_29_scale_0 = const()[name = string("dequantize_29_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_29 = dequantize(input = quantize_29, scale = dequantize_29_scale_0)[name = string("dequantize_29")];
            tensor<fp16, [?, ?, 384]> linear_13_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_key_bias_to_fp16, weight = model_encoder_layer_2_attention_self_key_weight_to_fp16_quantized, x = dequantize_29)[name = string("linear_13_cast_fp16")];
            fp16 quantize_93_scale_0 = const()[name = string("quantize_93_scale_0"), val = fp16(nan)];
            string quantize_93_output_dtype_0 = const()[name = string("quantize_93_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_137 = quantize(input = linear_13_cast_fp16, output_dtype = quantize_93_output_dtype_0, scale = quantize_93_scale_0)[name = string("quantize_137")];
            fp16 dequantize_93_scale_0 = const()[name = string("dequantize_93_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_137 = dequantize(input = quantize_137, scale = dequantize_93_scale_0)[name = string("dequantize_137")];
            tensor<int32, [3]> var_254_shape_cast_fp16 = shape(x = dequantize_137)[name = string("op_254_shape_cast_fp16")];
            int32 gather_17_axis_0 = const()[name = string("gather_17_axis_0"), val = int32(0)];
            int32 gather_17_batch_dims_0 = const()[name = string("gather_17_batch_dims_0"), val = int32(0)];
            bool gather_17_validate_indices_0 = const()[name = string("gather_17_validate_indices_0"), val = bool(false)];
            string var_254_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_254_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_17_indices_0_to_uint16 = const()[name = string("gather_17_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_254_shape_cast_fp16_to_uint16 = cast(dtype = var_254_shape_cast_fp16_to_uint16_dtype_0, x = var_254_shape_cast_fp16)[name = string("cast_47")];
            uint16 gather_17_cast_uint16 = gather(axis = gather_17_axis_0, batch_dims = gather_17_batch_dims_0, indices = gather_17_indices_0_to_uint16, validate_indices = gather_17_validate_indices_0, x = var_254_shape_cast_fp16_to_uint16)[name = string("gather_17_cast_uint16")];
            string gather_17_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_17_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_18_axis_0 = const()[name = string("gather_18_axis_0"), val = int32(0)];
            int32 gather_18_batch_dims_0 = const()[name = string("gather_18_batch_dims_0"), val = int32(0)];
            bool gather_18_validate_indices_0 = const()[name = string("gather_18_validate_indices_0"), val = bool(false)];
            uint16 gather_18_indices_0_to_uint16 = const()[name = string("gather_18_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_18_cast_uint16 = gather(axis = gather_18_axis_0, batch_dims = gather_18_batch_dims_0, indices = gather_18_indices_0_to_uint16, validate_indices = gather_18_validate_indices_0, x = var_254_shape_cast_fp16_to_uint16)[name = string("gather_18_cast_uint16")];
            string gather_18_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_18_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_9_axis_0 = const()[name = string("concat_9_axis_0"), val = int32(0)];
            bool concat_9_interleave_0 = const()[name = string("concat_9_interleave_0"), val = bool(false)];
            int32 gather_18_cast_uint16_to_int32 = cast(dtype = gather_18_cast_uint16_to_int32_dtype_0, x = gather_18_cast_uint16)[name = string("cast_45")];
            int32 gather_17_cast_uint16_to_int32 = cast(dtype = gather_17_cast_uint16_to_int32_dtype_0, x = gather_17_cast_uint16)[name = string("cast_46")];
            tensor<int32, [4]> concat_9 = concat(axis = concat_9_axis_0, interleave = concat_9_interleave_0, values = (gather_17_cast_uint16_to_int32, gather_18_cast_uint16_to_int32, var_23, var_22))[name = string("concat_9")];
            tensor<fp16, [?, ?, 12, 32]> x_27_cast_fp16 = reshape(shape = concat_9, x = dequantize_137)[name = string("x_27_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_value_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(15858752))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16006272))))[name = string("model_encoder_layer_2_attention_self_value_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_value_bias_to_fp16 = const()[name = string("model_encoder_layer_2_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16007104)))];
            fp16 quantize_30_scale_0 = const()[name = string("quantize_30_scale_0"), val = fp16(nan)];
            string quantize_30_output_dtype_0 = const()[name = string("quantize_30_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_30 = quantize(input = input_53_cast_fp16, output_dtype = quantize_30_output_dtype_0, scale = quantize_30_scale_0)[name = string("quantize_30")];
            fp16 dequantize_30_scale_0 = const()[name = string("dequantize_30_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_30 = dequantize(input = quantize_30, scale = dequantize_30_scale_0)[name = string("dequantize_30")];
            tensor<fp16, [?, ?, 384]> linear_14_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_value_bias_to_fp16, weight = model_encoder_layer_2_attention_self_value_weight_to_fp16_quantized, x = dequantize_30)[name = string("linear_14_cast_fp16")];
            fp16 quantize_94_scale_0 = const()[name = string("quantize_94_scale_0"), val = fp16(nan)];
            string quantize_94_output_dtype_0 = const()[name = string("quantize_94_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_138 = quantize(input = linear_14_cast_fp16, output_dtype = quantize_94_output_dtype_0, scale = quantize_94_scale_0)[name = string("quantize_138")];
            fp16 dequantize_94_scale_0 = const()[name = string("dequantize_94_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_138 = dequantize(input = quantize_138, scale = dequantize_94_scale_0)[name = string("dequantize_138")];
            tensor<int32, [3]> var_263_shape_cast_fp16 = shape(x = dequantize_138)[name = string("op_263_shape_cast_fp16")];
            int32 gather_19_axis_0 = const()[name = string("gather_19_axis_0"), val = int32(0)];
            int32 gather_19_batch_dims_0 = const()[name = string("gather_19_batch_dims_0"), val = int32(0)];
            bool gather_19_validate_indices_0 = const()[name = string("gather_19_validate_indices_0"), val = bool(false)];
            string var_263_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_263_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_19_indices_0_to_uint16 = const()[name = string("gather_19_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_263_shape_cast_fp16_to_uint16 = cast(dtype = var_263_shape_cast_fp16_to_uint16_dtype_0, x = var_263_shape_cast_fp16)[name = string("cast_44")];
            uint16 gather_19_cast_uint16 = gather(axis = gather_19_axis_0, batch_dims = gather_19_batch_dims_0, indices = gather_19_indices_0_to_uint16, validate_indices = gather_19_validate_indices_0, x = var_263_shape_cast_fp16_to_uint16)[name = string("gather_19_cast_uint16")];
            string gather_19_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_19_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_20_axis_0 = const()[name = string("gather_20_axis_0"), val = int32(0)];
            int32 gather_20_batch_dims_0 = const()[name = string("gather_20_batch_dims_0"), val = int32(0)];
            bool gather_20_validate_indices_0 = const()[name = string("gather_20_validate_indices_0"), val = bool(false)];
            uint16 gather_20_indices_0_to_uint16 = const()[name = string("gather_20_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_20_cast_uint16 = gather(axis = gather_20_axis_0, batch_dims = gather_20_batch_dims_0, indices = gather_20_indices_0_to_uint16, validate_indices = gather_20_validate_indices_0, x = var_263_shape_cast_fp16_to_uint16)[name = string("gather_20_cast_uint16")];
            string gather_20_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_20_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_10_axis_0 = const()[name = string("concat_10_axis_0"), val = int32(0)];
            bool concat_10_interleave_0 = const()[name = string("concat_10_interleave_0"), val = bool(false)];
            int32 gather_20_cast_uint16_to_int32 = cast(dtype = gather_20_cast_uint16_to_int32_dtype_0, x = gather_20_cast_uint16)[name = string("cast_42")];
            int32 gather_19_cast_uint16_to_int32 = cast(dtype = gather_19_cast_uint16_to_int32_dtype_0, x = gather_19_cast_uint16)[name = string("cast_43")];
            tensor<int32, [4]> concat_10 = concat(axis = concat_10_axis_0, interleave = concat_10_interleave_0, values = (gather_19_cast_uint16_to_int32, gather_20_cast_uint16_to_int32, var_23, var_22))[name = string("concat_10")];
            tensor<fp16, [?, ?, 12, 32]> x_31_cast_fp16 = reshape(shape = concat_10, x = dequantize_138)[name = string("x_31_cast_fp16")];
            tensor<int32, [4]> var_267 = const()[name = string("op_267"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [3]> var_269_shape_cast_fp16 = shape(x = dequantize_136)[name = string("op_269_shape_cast_fp16")];
            int32 gather_21_axis_0 = const()[name = string("gather_21_axis_0"), val = int32(0)];
            int32 gather_21_batch_dims_0 = const()[name = string("gather_21_batch_dims_0"), val = int32(0)];
            bool gather_21_validate_indices_0 = const()[name = string("gather_21_validate_indices_0"), val = bool(false)];
            string var_269_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_269_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_21_indices_0_to_uint16 = const()[name = string("gather_21_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_269_shape_cast_fp16_to_uint16 = cast(dtype = var_269_shape_cast_fp16_to_uint16_dtype_0, x = var_269_shape_cast_fp16)[name = string("cast_41")];
            uint16 gather_21_cast_uint16 = gather(axis = gather_21_axis_0, batch_dims = gather_21_batch_dims_0, indices = gather_21_indices_0_to_uint16, validate_indices = gather_21_validate_indices_0, x = var_269_shape_cast_fp16_to_uint16)[name = string("gather_21_cast_uint16")];
            string gather_21_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_21_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_22_axis_0 = const()[name = string("gather_22_axis_0"), val = int32(0)];
            int32 gather_22_batch_dims_0 = const()[name = string("gather_22_batch_dims_0"), val = int32(0)];
            bool gather_22_validate_indices_0 = const()[name = string("gather_22_validate_indices_0"), val = bool(false)];
            uint16 gather_22_indices_0_to_uint16 = const()[name = string("gather_22_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_22_cast_uint16 = gather(axis = gather_22_axis_0, batch_dims = gather_22_batch_dims_0, indices = gather_22_indices_0_to_uint16, validate_indices = gather_22_validate_indices_0, x = var_269_shape_cast_fp16_to_uint16)[name = string("gather_22_cast_uint16")];
            string gather_22_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_22_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_11_axis_0 = const()[name = string("concat_11_axis_0"), val = int32(0)];
            bool concat_11_interleave_0 = const()[name = string("concat_11_interleave_0"), val = bool(false)];
            int32 gather_22_cast_uint16_to_int32 = cast(dtype = gather_22_cast_uint16_to_int32_dtype_0, x = gather_22_cast_uint16)[name = string("cast_39")];
            int32 gather_21_cast_uint16_to_int32 = cast(dtype = gather_21_cast_uint16_to_int32_dtype_0, x = gather_21_cast_uint16)[name = string("cast_40")];
            tensor<int32, [4]> concat_11 = concat(axis = concat_11_axis_0, interleave = concat_11_interleave_0, values = (gather_21_cast_uint16_to_int32, gather_22_cast_uint16_to_int32, var_23, var_22))[name = string("concat_11")];
            tensor<fp16, [?, ?, 12, 32]> x_35_cast_fp16 = reshape(shape = concat_11, x = dequantize_136)[name = string("x_35_cast_fp16")];
            bool attention_scores_9_transpose_x_0 = const()[name = string("attention_scores_9_transpose_x_0"), val = bool(false)];
            bool attention_scores_9_transpose_y_0 = const()[name = string("attention_scores_9_transpose_y_0"), val = bool(false)];
            tensor<int32, [4]> transpose_28_perm_0 = const()[name = string("transpose_28_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_29_perm_0 = const()[name = string("transpose_29_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [?, 12, 32, ?]> transpose_29 = transpose(perm = transpose_29_perm_0, x = x_27_cast_fp16)[name = string("transpose_50")];
            tensor<fp16, [?, 12, ?, 32]> transpose_28 = transpose(perm = transpose_28_perm_0, x = x_35_cast_fp16)[name = string("transpose_51")];
            tensor<fp16, [?, 12, ?, ?]> attention_scores_9_cast_fp16 = matmul(transpose_x = attention_scores_9_transpose_x_0, transpose_y = attention_scores_9_transpose_y_0, x = transpose_28, y = transpose_29)[name = string("attention_scores_9_cast_fp16")];
            fp16 _inversed_attention_scores_11_y_0_to_fp16 = const()[name = string("_inversed_attention_scores_11_y_0_to_fp16"), val = fp16(0x1.6ap-3)];
            tensor<fp16, [?, 12, ?, ?]> _inversed_attention_scores_11_cast_fp16 = mul(x = attention_scores_9_cast_fp16, y = _inversed_attention_scores_11_y_0_to_fp16)[name = string("_inversed_attention_scores_11_cast_fp16")];
            fp16 quantize_31_scale_0 = const()[name = string("quantize_31_scale_0"), val = fp16(nan)];
            string quantize_31_output_dtype_0 = const()[name = string("quantize_31_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 12, ?, ?]> quantize_31 = quantize(input = _inversed_attention_scores_11_cast_fp16, output_dtype = quantize_31_output_dtype_0, scale = quantize_31_scale_0)[name = string("quantize_31")];
            fp16 quantize_32_scale_0 = const()[name = string("quantize_32_scale_0"), val = fp16(nan)];
            string quantize_32_output_dtype_0 = const()[name = string("quantize_32_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 1, 1, ?]> quantize_32 = quantize(input = attention_mask_cast_fp16, output_dtype = quantize_32_output_dtype_0, scale = quantize_32_scale_0)[name = string("quantize_32")];
            fp16 dequantize_182_scale_0 = const()[name = string("dequantize_182_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, 1, 1, ?]> dequantize_182 = dequantize(input = quantize_32, scale = dequantize_182_scale_0)[name = string("dequantize_182")];
            fp16 dequantize_16_scale_0_1 = const()[name = string("dequantize_16_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_16_1 = dequantize(input = quantize_31, scale = dequantize_16_scale_0_1)[name = string("dequantize_16_1")];
            tensor<fp16, [?, 12, ?, ?]> input_55_cast_fp16 = add(x = dequantize_16_1, y = dequantize_182)[name = string("input_55_cast_fp16")];
            string quantize_95_output_dtype_0 = const()[name = string("quantize_95_output_dtype_0"), val = string("int8")];
            fp16 quantize_8_scale_0_1 = const()[name = string("quantize_8_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, 12, ?, ?]> quantize_8_1 = quantize(input = input_55_cast_fp16, output_dtype = quantize_95_output_dtype_0, scale = quantize_8_scale_0_1)[name = string("quantize_8_1")];
            fp16 dequantize_95_scale_0 = const()[name = string("dequantize_95_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_139 = dequantize(input = quantize_8_1, scale = dequantize_95_scale_0)[name = string("dequantize_139")];
            tensor<fp16, [?, 12, ?, ?]> input_57_cast_fp16 = softmax(axis = var_24, x = dequantize_139)[name = string("input_57_cast_fp16")];
            bool context_layer_9_transpose_x_0 = const()[name = string("context_layer_9_transpose_x_0"), val = bool(false)];
            bool context_layer_9_transpose_y_0 = const()[name = string("context_layer_9_transpose_y_0"), val = bool(false)];
            tensor<fp16, [?, 12, ?, 32]> value_layer_5_cast_fp16 = transpose(perm = var_267, x = x_31_cast_fp16)[name = string("transpose_49")];
            tensor<fp16, [?, 12, ?, 32]> context_layer_9_cast_fp16 = matmul(transpose_x = context_layer_9_transpose_x_0, transpose_y = context_layer_9_transpose_y_0, x = input_57_cast_fp16, y = value_layer_5_cast_fp16)[name = string("context_layer_9_cast_fp16")];
            tensor<int32, [4]> var_283 = const()[name = string("op_283"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [?, ?, 12, 32]> var_284_cast_fp16 = transpose(perm = var_283, x = context_layer_9_cast_fp16)[name = string("transpose_48")];
            tensor<int32, [4]> var_286_shape_cast_fp16 = shape(x = var_284_cast_fp16)[name = string("op_286_shape_cast_fp16")];
            int32 gather_23_axis_0 = const()[name = string("gather_23_axis_0"), val = int32(0)];
            int32 gather_23_batch_dims_0 = const()[name = string("gather_23_batch_dims_0"), val = int32(0)];
            bool gather_23_validate_indices_0 = const()[name = string("gather_23_validate_indices_0"), val = bool(false)];
            string var_286_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_286_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_23_indices_0_to_uint16 = const()[name = string("gather_23_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [4]> var_286_shape_cast_fp16_to_uint16 = cast(dtype = var_286_shape_cast_fp16_to_uint16_dtype_0, x = var_286_shape_cast_fp16)[name = string("cast_38")];
            uint16 gather_23_cast_uint16 = gather(axis = gather_23_axis_0, batch_dims = gather_23_batch_dims_0, indices = gather_23_indices_0_to_uint16, validate_indices = gather_23_validate_indices_0, x = var_286_shape_cast_fp16_to_uint16)[name = string("gather_23_cast_uint16")];
            string gather_23_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_23_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_24_axis_0 = const()[name = string("gather_24_axis_0"), val = int32(0)];
            int32 gather_24_batch_dims_0 = const()[name = string("gather_24_batch_dims_0"), val = int32(0)];
            bool gather_24_validate_indices_0 = const()[name = string("gather_24_validate_indices_0"), val = bool(false)];
            uint16 gather_24_indices_0_to_uint16 = const()[name = string("gather_24_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_24_cast_uint16 = gather(axis = gather_24_axis_0, batch_dims = gather_24_batch_dims_0, indices = gather_24_indices_0_to_uint16, validate_indices = gather_24_validate_indices_0, x = var_286_shape_cast_fp16_to_uint16)[name = string("gather_24_cast_uint16")];
            string gather_24_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_24_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_12_axis_0 = const()[name = string("concat_12_axis_0"), val = int32(0)];
            bool concat_12_interleave_0 = const()[name = string("concat_12_interleave_0"), val = bool(false)];
            int32 gather_24_cast_uint16_to_int32 = cast(dtype = gather_24_cast_uint16_to_int32_dtype_0, x = gather_24_cast_uint16)[name = string("cast_36")];
            int32 gather_23_cast_uint16_to_int32 = cast(dtype = gather_23_cast_uint16_to_int32_dtype_0, x = gather_23_cast_uint16)[name = string("cast_37")];
            tensor<int32, [3]> concat_12 = concat(axis = concat_12_axis_0, interleave = concat_12_interleave_0, values = (gather_23_cast_uint16_to_int32, gather_24_cast_uint16_to_int32, var_27))[name = string("concat_12")];
            tensor<fp16, [?, ?, 384]> input_59_cast_fp16 = reshape(shape = concat_12, x = var_284_cast_fp16)[name = string("input_59_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16007936))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16155456))))[name = string("model_encoder_layer_2_attention_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_2_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16156288)))];
            fp16 quantize_33_scale_0 = const()[name = string("quantize_33_scale_0"), val = fp16(nan)];
            string quantize_33_output_dtype_0 = const()[name = string("quantize_33_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_33 = quantize(input = input_59_cast_fp16, output_dtype = quantize_33_output_dtype_0, scale = quantize_33_scale_0)[name = string("quantize_33")];
            fp16 dequantize_33_scale_0 = const()[name = string("dequantize_33_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_33 = dequantize(input = quantize_33, scale = dequantize_33_scale_0)[name = string("dequantize_33")];
            tensor<fp16, [?, ?, 384]> linear_15_cast_fp16 = linear(bias = model_encoder_layer_2_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_2_attention_output_dense_weight_to_fp16_quantized, x = dequantize_33)[name = string("linear_15_cast_fp16")];
            fp16 quantize_34_scale_0 = const()[name = string("quantize_34_scale_0"), val = fp16(nan)];
            string quantize_34_output_dtype_0 = const()[name = string("quantize_34_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_34 = quantize(input = linear_15_cast_fp16, output_dtype = quantize_34_output_dtype_0, scale = quantize_34_scale_0)[name = string("quantize_34")];
            fp16 quantize_35_scale_0 = const()[name = string("quantize_35_scale_0"), val = fp16(nan)];
            string quantize_35_output_dtype_0 = const()[name = string("quantize_35_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_35 = quantize(input = input_53_cast_fp16, output_dtype = quantize_35_output_dtype_0, scale = quantize_35_scale_0)[name = string("quantize_35")];
            fp16 dequantize_184_scale_0 = const()[name = string("dequantize_184_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_184 = dequantize(input = quantize_35, scale = dequantize_184_scale_0)[name = string("dequantize_184")];
            fp16 dequantize_18_scale_0_1 = const()[name = string("dequantize_18_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_18_1 = dequantize(input = quantize_34, scale = dequantize_18_scale_0_1)[name = string("dequantize_18_1")];
            tensor<fp16, [?, ?, 384]> input_63_cast_fp16 = add(x = dequantize_18_1, y = dequantize_184)[name = string("input_63_cast_fp16")];
            string quantize_96_output_dtype_0 = const()[name = string("quantize_96_output_dtype_0"), val = string("int8")];
            fp16 quantize_9_scale_0_1 = const()[name = string("quantize_9_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_9_1 = quantize(input = input_63_cast_fp16, output_dtype = quantize_96_output_dtype_0, scale = quantize_9_scale_0_1)[name = string("quantize_9_1")];
            fp16 dequantize_96_scale_0 = const()[name = string("dequantize_96_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_140 = dequantize(input = quantize_9_1, scale = dequantize_96_scale_0)[name = string("dequantize_140")];
            tensor<int32, [1]> input_65_axes_0 = const()[name = string("input_65_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16157120)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16157952)))];
            tensor<fp16, [?, ?, 384]> input_65_cast_fp16 = layer_norm(axes = input_65_axes_0, beta = model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16, x = dequantize_140)[name = string("input_65_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_2_intermediate_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [1536, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16158784))), scale = tensor<fp16, [1536, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16748672))))[name = string("model_encoder_layer_2_intermediate_dense_weight_to_fp16_quantized")];
            tensor<fp16, [1536]> model_encoder_layer_2_intermediate_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_2_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16751808)))];
            fp16 quantize_36_scale_0 = const()[name = string("quantize_36_scale_0"), val = fp16(nan)];
            string quantize_36_output_dtype_0 = const()[name = string("quantize_36_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_36 = quantize(input = input_65_cast_fp16, output_dtype = quantize_36_output_dtype_0, scale = quantize_36_scale_0)[name = string("quantize_36")];
            fp16 dequantize_36_scale_0 = const()[name = string("dequantize_36_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_36 = dequantize(input = quantize_36, scale = dequantize_36_scale_0)[name = string("dequantize_36")];
            tensor<fp16, [?, ?, 1536]> linear_16_cast_fp16 = linear(bias = model_encoder_layer_2_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_2_intermediate_dense_weight_to_fp16_quantized, x = dequantize_36)[name = string("linear_16_cast_fp16")];
            fp16 quantize_97_scale_0 = const()[name = string("quantize_97_scale_0"), val = fp16(nan)];
            string quantize_97_output_dtype_0 = const()[name = string("quantize_97_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_141 = quantize(input = linear_16_cast_fp16, output_dtype = quantize_97_output_dtype_0, scale = quantize_97_scale_0)[name = string("quantize_141")];
            fp16 dequantize_97_scale_0 = const()[name = string("dequantize_97_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_141 = dequantize(input = quantize_141, scale = dequantize_97_scale_0)[name = string("dequantize_141")];
            string input_69_mode_0 = const()[name = string("input_69_mode_0"), val = string("EXACT")];
            tensor<fp16, [?, ?, 1536]> input_69_cast_fp16 = gelu(mode = input_69_mode_0, x = dequantize_141)[name = string("input_69_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_2_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(16754944))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17344832))))[name = string("model_encoder_layer_2_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_2_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_2_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17345664)))];
            fp16 quantize_37_scale_0 = const()[name = string("quantize_37_scale_0"), val = fp16(nan)];
            string quantize_37_output_dtype_0 = const()[name = string("quantize_37_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_37 = quantize(input = input_69_cast_fp16, output_dtype = quantize_37_output_dtype_0, scale = quantize_37_scale_0)[name = string("quantize_37")];
            fp16 dequantize_37_scale_0 = const()[name = string("dequantize_37_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_37 = dequantize(input = quantize_37, scale = dequantize_37_scale_0)[name = string("dequantize_37")];
            tensor<fp16, [?, ?, 384]> linear_17_cast_fp16 = linear(bias = model_encoder_layer_2_output_dense_bias_to_fp16, weight = model_encoder_layer_2_output_dense_weight_to_fp16_quantized, x = dequantize_37)[name = string("linear_17_cast_fp16")];
            fp16 quantize_38_scale_0 = const()[name = string("quantize_38_scale_0"), val = fp16(nan)];
            string quantize_38_output_dtype_0 = const()[name = string("quantize_38_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_38 = quantize(input = linear_17_cast_fp16, output_dtype = quantize_38_output_dtype_0, scale = quantize_38_scale_0)[name = string("quantize_38")];
            fp16 quantize_39_scale_0 = const()[name = string("quantize_39_scale_0"), val = fp16(nan)];
            string quantize_39_output_dtype_0 = const()[name = string("quantize_39_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_39 = quantize(input = input_65_cast_fp16, output_dtype = quantize_39_output_dtype_0, scale = quantize_39_scale_0)[name = string("quantize_39")];
            fp16 dequantize_186_scale_0 = const()[name = string("dequantize_186_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_186 = dequantize(input = quantize_39, scale = dequantize_186_scale_0)[name = string("dequantize_186")];
            fp16 dequantize_20_scale_0 = const()[name = string("dequantize_20_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_20 = dequantize(input = quantize_38, scale = dequantize_20_scale_0)[name = string("dequantize_20")];
            tensor<fp16, [?, ?, 384]> input_73_cast_fp16 = add(x = dequantize_20, y = dequantize_186)[name = string("input_73_cast_fp16")];
            string quantize_98_output_dtype_0 = const()[name = string("quantize_98_output_dtype_0"), val = string("int8")];
            fp16 quantize_10_scale_0_1 = const()[name = string("quantize_10_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_10_1 = quantize(input = input_73_cast_fp16, output_dtype = quantize_98_output_dtype_0, scale = quantize_10_scale_0_1)[name = string("quantize_10_1")];
            fp16 dequantize_98_scale_0 = const()[name = string("dequantize_98_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_142 = dequantize(input = quantize_10_1, scale = dequantize_98_scale_0)[name = string("dequantize_142")];
            tensor<int32, [1]> input_75_axes_0 = const()[name = string("input_75_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_2_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17346496)))];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_2_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17347328)))];
            tensor<fp16, [?, ?, 384]> input_75_cast_fp16 = layer_norm(axes = input_75_axes_0, beta = model_encoder_layer_2_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_2_output_LayerNorm_weight_to_fp16, x = dequantize_142)[name = string("input_75_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_query_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17348160))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17495680))))[name = string("model_encoder_layer_3_attention_self_query_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_query_bias_to_fp16 = const()[name = string("model_encoder_layer_3_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17496512)))];
            fp16 quantize_40_scale_0 = const()[name = string("quantize_40_scale_0"), val = fp16(nan)];
            string quantize_40_output_dtype_0 = const()[name = string("quantize_40_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_40 = quantize(input = input_75_cast_fp16, output_dtype = quantize_40_output_dtype_0, scale = quantize_40_scale_0)[name = string("quantize_40")];
            fp16 dequantize_40_scale_0 = const()[name = string("dequantize_40_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_40 = dequantize(input = quantize_40, scale = dequantize_40_scale_0)[name = string("dequantize_40")];
            tensor<fp16, [?, ?, 384]> linear_18_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_query_bias_to_fp16, weight = model_encoder_layer_3_attention_self_query_weight_to_fp16_quantized, x = dequantize_40)[name = string("linear_18_cast_fp16")];
            fp16 quantize_99_scale_0 = const()[name = string("quantize_99_scale_0"), val = fp16(nan)];
            string quantize_99_output_dtype_0 = const()[name = string("quantize_99_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_143 = quantize(input = linear_18_cast_fp16, output_dtype = quantize_99_output_dtype_0, scale = quantize_99_scale_0)[name = string("quantize_143")];
            fp16 dequantize_99_scale_0 = const()[name = string("dequantize_99_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_143 = dequantize(input = quantize_143, scale = dequantize_99_scale_0)[name = string("dequantize_143")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_key_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17497344))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17644864))))[name = string("model_encoder_layer_3_attention_self_key_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_key_bias_to_fp16 = const()[name = string("model_encoder_layer_3_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17645696)))];
            fp16 quantize_41_scale_0 = const()[name = string("quantize_41_scale_0"), val = fp16(nan)];
            string quantize_41_output_dtype_0 = const()[name = string("quantize_41_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_41 = quantize(input = input_75_cast_fp16, output_dtype = quantize_41_output_dtype_0, scale = quantize_41_scale_0)[name = string("quantize_41")];
            fp16 dequantize_41_scale_0 = const()[name = string("dequantize_41_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_41 = dequantize(input = quantize_41, scale = dequantize_41_scale_0)[name = string("dequantize_41")];
            tensor<fp16, [?, ?, 384]> linear_19_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_key_bias_to_fp16, weight = model_encoder_layer_3_attention_self_key_weight_to_fp16_quantized, x = dequantize_41)[name = string("linear_19_cast_fp16")];
            fp16 quantize_100_scale_0 = const()[name = string("quantize_100_scale_0"), val = fp16(nan)];
            string quantize_100_output_dtype_0 = const()[name = string("quantize_100_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_144 = quantize(input = linear_19_cast_fp16, output_dtype = quantize_100_output_dtype_0, scale = quantize_100_scale_0)[name = string("quantize_144")];
            fp16 dequantize_100_scale_0 = const()[name = string("dequantize_100_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_144 = dequantize(input = quantize_144, scale = dequantize_100_scale_0)[name = string("dequantize_144")];
            tensor<int32, [3]> var_331_shape_cast_fp16 = shape(x = dequantize_144)[name = string("op_331_shape_cast_fp16")];
            int32 gather_25_axis_0 = const()[name = string("gather_25_axis_0"), val = int32(0)];
            int32 gather_25_batch_dims_0 = const()[name = string("gather_25_batch_dims_0"), val = int32(0)];
            bool gather_25_validate_indices_0 = const()[name = string("gather_25_validate_indices_0"), val = bool(false)];
            string var_331_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_331_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_25_indices_0_to_uint16 = const()[name = string("gather_25_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_331_shape_cast_fp16_to_uint16 = cast(dtype = var_331_shape_cast_fp16_to_uint16_dtype_0, x = var_331_shape_cast_fp16)[name = string("cast_35")];
            uint16 gather_25_cast_uint16 = gather(axis = gather_25_axis_0, batch_dims = gather_25_batch_dims_0, indices = gather_25_indices_0_to_uint16, validate_indices = gather_25_validate_indices_0, x = var_331_shape_cast_fp16_to_uint16)[name = string("gather_25_cast_uint16")];
            string gather_25_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_25_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_26_axis_0 = const()[name = string("gather_26_axis_0"), val = int32(0)];
            int32 gather_26_batch_dims_0 = const()[name = string("gather_26_batch_dims_0"), val = int32(0)];
            bool gather_26_validate_indices_0 = const()[name = string("gather_26_validate_indices_0"), val = bool(false)];
            uint16 gather_26_indices_0_to_uint16 = const()[name = string("gather_26_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_26_cast_uint16 = gather(axis = gather_26_axis_0, batch_dims = gather_26_batch_dims_0, indices = gather_26_indices_0_to_uint16, validate_indices = gather_26_validate_indices_0, x = var_331_shape_cast_fp16_to_uint16)[name = string("gather_26_cast_uint16")];
            string gather_26_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_26_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_13_axis_0 = const()[name = string("concat_13_axis_0"), val = int32(0)];
            bool concat_13_interleave_0 = const()[name = string("concat_13_interleave_0"), val = bool(false)];
            int32 gather_26_cast_uint16_to_int32 = cast(dtype = gather_26_cast_uint16_to_int32_dtype_0, x = gather_26_cast_uint16)[name = string("cast_33")];
            int32 gather_25_cast_uint16_to_int32 = cast(dtype = gather_25_cast_uint16_to_int32_dtype_0, x = gather_25_cast_uint16)[name = string("cast_34")];
            tensor<int32, [4]> concat_13 = concat(axis = concat_13_axis_0, interleave = concat_13_interleave_0, values = (gather_25_cast_uint16_to_int32, gather_26_cast_uint16_to_int32, var_23, var_22))[name = string("concat_13")];
            tensor<fp16, [?, ?, 12, 32]> x_39_cast_fp16 = reshape(shape = concat_13, x = dequantize_144)[name = string("x_39_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_value_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17646528))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17794048))))[name = string("model_encoder_layer_3_attention_self_value_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_value_bias_to_fp16 = const()[name = string("model_encoder_layer_3_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17794880)))];
            fp16 quantize_42_scale_0 = const()[name = string("quantize_42_scale_0"), val = fp16(nan)];
            string quantize_42_output_dtype_0 = const()[name = string("quantize_42_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_42 = quantize(input = input_75_cast_fp16, output_dtype = quantize_42_output_dtype_0, scale = quantize_42_scale_0)[name = string("quantize_42")];
            fp16 dequantize_42_scale_0 = const()[name = string("dequantize_42_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_42 = dequantize(input = quantize_42, scale = dequantize_42_scale_0)[name = string("dequantize_42")];
            tensor<fp16, [?, ?, 384]> linear_20_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_value_bias_to_fp16, weight = model_encoder_layer_3_attention_self_value_weight_to_fp16_quantized, x = dequantize_42)[name = string("linear_20_cast_fp16")];
            fp16 quantize_101_scale_0 = const()[name = string("quantize_101_scale_0"), val = fp16(nan)];
            string quantize_101_output_dtype_0 = const()[name = string("quantize_101_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_145 = quantize(input = linear_20_cast_fp16, output_dtype = quantize_101_output_dtype_0, scale = quantize_101_scale_0)[name = string("quantize_145")];
            fp16 dequantize_101_scale_0 = const()[name = string("dequantize_101_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_145 = dequantize(input = quantize_145, scale = dequantize_101_scale_0)[name = string("dequantize_145")];
            tensor<int32, [3]> var_340_shape_cast_fp16 = shape(x = dequantize_145)[name = string("op_340_shape_cast_fp16")];
            int32 gather_27_axis_0 = const()[name = string("gather_27_axis_0"), val = int32(0)];
            int32 gather_27_batch_dims_0 = const()[name = string("gather_27_batch_dims_0"), val = int32(0)];
            bool gather_27_validate_indices_0 = const()[name = string("gather_27_validate_indices_0"), val = bool(false)];
            string var_340_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_340_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_27_indices_0_to_uint16 = const()[name = string("gather_27_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_340_shape_cast_fp16_to_uint16 = cast(dtype = var_340_shape_cast_fp16_to_uint16_dtype_0, x = var_340_shape_cast_fp16)[name = string("cast_32")];
            uint16 gather_27_cast_uint16 = gather(axis = gather_27_axis_0, batch_dims = gather_27_batch_dims_0, indices = gather_27_indices_0_to_uint16, validate_indices = gather_27_validate_indices_0, x = var_340_shape_cast_fp16_to_uint16)[name = string("gather_27_cast_uint16")];
            string gather_27_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_27_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_28_axis_0 = const()[name = string("gather_28_axis_0"), val = int32(0)];
            int32 gather_28_batch_dims_0 = const()[name = string("gather_28_batch_dims_0"), val = int32(0)];
            bool gather_28_validate_indices_0 = const()[name = string("gather_28_validate_indices_0"), val = bool(false)];
            uint16 gather_28_indices_0_to_uint16 = const()[name = string("gather_28_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_28_cast_uint16 = gather(axis = gather_28_axis_0, batch_dims = gather_28_batch_dims_0, indices = gather_28_indices_0_to_uint16, validate_indices = gather_28_validate_indices_0, x = var_340_shape_cast_fp16_to_uint16)[name = string("gather_28_cast_uint16")];
            string gather_28_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_28_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_14_axis_0 = const()[name = string("concat_14_axis_0"), val = int32(0)];
            bool concat_14_interleave_0 = const()[name = string("concat_14_interleave_0"), val = bool(false)];
            int32 gather_28_cast_uint16_to_int32 = cast(dtype = gather_28_cast_uint16_to_int32_dtype_0, x = gather_28_cast_uint16)[name = string("cast_30")];
            int32 gather_27_cast_uint16_to_int32 = cast(dtype = gather_27_cast_uint16_to_int32_dtype_0, x = gather_27_cast_uint16)[name = string("cast_31")];
            tensor<int32, [4]> concat_14 = concat(axis = concat_14_axis_0, interleave = concat_14_interleave_0, values = (gather_27_cast_uint16_to_int32, gather_28_cast_uint16_to_int32, var_23, var_22))[name = string("concat_14")];
            tensor<fp16, [?, ?, 12, 32]> x_43_cast_fp16 = reshape(shape = concat_14, x = dequantize_145)[name = string("x_43_cast_fp16")];
            tensor<int32, [4]> var_344 = const()[name = string("op_344"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [3]> var_346_shape_cast_fp16 = shape(x = dequantize_143)[name = string("op_346_shape_cast_fp16")];
            int32 gather_29_axis_0 = const()[name = string("gather_29_axis_0"), val = int32(0)];
            int32 gather_29_batch_dims_0 = const()[name = string("gather_29_batch_dims_0"), val = int32(0)];
            bool gather_29_validate_indices_0 = const()[name = string("gather_29_validate_indices_0"), val = bool(false)];
            string var_346_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_346_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_29_indices_0_to_uint16 = const()[name = string("gather_29_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_346_shape_cast_fp16_to_uint16 = cast(dtype = var_346_shape_cast_fp16_to_uint16_dtype_0, x = var_346_shape_cast_fp16)[name = string("cast_29")];
            uint16 gather_29_cast_uint16 = gather(axis = gather_29_axis_0, batch_dims = gather_29_batch_dims_0, indices = gather_29_indices_0_to_uint16, validate_indices = gather_29_validate_indices_0, x = var_346_shape_cast_fp16_to_uint16)[name = string("gather_29_cast_uint16")];
            string gather_29_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_29_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_30_axis_0 = const()[name = string("gather_30_axis_0"), val = int32(0)];
            int32 gather_30_batch_dims_0 = const()[name = string("gather_30_batch_dims_0"), val = int32(0)];
            bool gather_30_validate_indices_0 = const()[name = string("gather_30_validate_indices_0"), val = bool(false)];
            uint16 gather_30_indices_0_to_uint16 = const()[name = string("gather_30_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_30_cast_uint16 = gather(axis = gather_30_axis_0, batch_dims = gather_30_batch_dims_0, indices = gather_30_indices_0_to_uint16, validate_indices = gather_30_validate_indices_0, x = var_346_shape_cast_fp16_to_uint16)[name = string("gather_30_cast_uint16")];
            string gather_30_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_30_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_15_axis_0 = const()[name = string("concat_15_axis_0"), val = int32(0)];
            bool concat_15_interleave_0 = const()[name = string("concat_15_interleave_0"), val = bool(false)];
            int32 gather_30_cast_uint16_to_int32 = cast(dtype = gather_30_cast_uint16_to_int32_dtype_0, x = gather_30_cast_uint16)[name = string("cast_27")];
            int32 gather_29_cast_uint16_to_int32 = cast(dtype = gather_29_cast_uint16_to_int32_dtype_0, x = gather_29_cast_uint16)[name = string("cast_28")];
            tensor<int32, [4]> concat_15 = concat(axis = concat_15_axis_0, interleave = concat_15_interleave_0, values = (gather_29_cast_uint16_to_int32, gather_30_cast_uint16_to_int32, var_23, var_22))[name = string("concat_15")];
            tensor<fp16, [?, ?, 12, 32]> x_47_cast_fp16 = reshape(shape = concat_15, x = dequantize_143)[name = string("x_47_cast_fp16")];
            bool attention_scores_13_transpose_x_0 = const()[name = string("attention_scores_13_transpose_x_0"), val = bool(false)];
            bool attention_scores_13_transpose_y_0 = const()[name = string("attention_scores_13_transpose_y_0"), val = bool(false)];
            tensor<int32, [4]> transpose_30_perm_0 = const()[name = string("transpose_30_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_31_perm_0 = const()[name = string("transpose_31_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [?, 12, 32, ?]> transpose_31 = transpose(perm = transpose_31_perm_0, x = x_39_cast_fp16)[name = string("transpose_46")];
            tensor<fp16, [?, 12, ?, 32]> transpose_30 = transpose(perm = transpose_30_perm_0, x = x_47_cast_fp16)[name = string("transpose_47")];
            tensor<fp16, [?, 12, ?, ?]> attention_scores_13_cast_fp16 = matmul(transpose_x = attention_scores_13_transpose_x_0, transpose_y = attention_scores_13_transpose_y_0, x = transpose_30, y = transpose_31)[name = string("attention_scores_13_cast_fp16")];
            fp16 _inversed_attention_scores_15_y_0_to_fp16 = const()[name = string("_inversed_attention_scores_15_y_0_to_fp16"), val = fp16(0x1.6ap-3)];
            tensor<fp16, [?, 12, ?, ?]> _inversed_attention_scores_15_cast_fp16 = mul(x = attention_scores_13_cast_fp16, y = _inversed_attention_scores_15_y_0_to_fp16)[name = string("_inversed_attention_scores_15_cast_fp16")];
            fp16 quantize_43_scale_0 = const()[name = string("quantize_43_scale_0"), val = fp16(nan)];
            string quantize_43_output_dtype_0 = const()[name = string("quantize_43_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 12, ?, ?]> quantize_43 = quantize(input = _inversed_attention_scores_15_cast_fp16, output_dtype = quantize_43_output_dtype_0, scale = quantize_43_scale_0)[name = string("quantize_43")];
            fp16 quantize_44_scale_0 = const()[name = string("quantize_44_scale_0"), val = fp16(nan)];
            string quantize_44_output_dtype_0 = const()[name = string("quantize_44_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 1, 1, ?]> quantize_44 = quantize(input = attention_mask_cast_fp16, output_dtype = quantize_44_output_dtype_0, scale = quantize_44_scale_0)[name = string("quantize_44")];
            fp16 dequantize_188_scale_0 = const()[name = string("dequantize_188_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, 1, 1, ?]> dequantize_188 = dequantize(input = quantize_44, scale = dequantize_188_scale_0)[name = string("dequantize_188")];
            fp16 dequantize_22_scale_0 = const()[name = string("dequantize_22_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_22 = dequantize(input = quantize_43, scale = dequantize_22_scale_0)[name = string("dequantize_22")];
            tensor<fp16, [?, 12, ?, ?]> input_77_cast_fp16 = add(x = dequantize_22, y = dequantize_188)[name = string("input_77_cast_fp16")];
            string quantize_102_output_dtype_0 = const()[name = string("quantize_102_output_dtype_0"), val = string("int8")];
            fp16 quantize_11_scale_0 = const()[name = string("quantize_11_scale_0"), val = fp16(nan)];
            tensor<int8, [?, 12, ?, ?]> quantize_11 = quantize(input = input_77_cast_fp16, output_dtype = quantize_102_output_dtype_0, scale = quantize_11_scale_0)[name = string("quantize_11")];
            fp16 dequantize_102_scale_0 = const()[name = string("dequantize_102_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_146 = dequantize(input = quantize_11, scale = dequantize_102_scale_0)[name = string("dequantize_146")];
            tensor<fp16, [?, 12, ?, ?]> input_79_cast_fp16 = softmax(axis = var_24, x = dequantize_146)[name = string("input_79_cast_fp16")];
            bool context_layer_13_transpose_x_0 = const()[name = string("context_layer_13_transpose_x_0"), val = bool(false)];
            bool context_layer_13_transpose_y_0 = const()[name = string("context_layer_13_transpose_y_0"), val = bool(false)];
            tensor<fp16, [?, 12, ?, 32]> value_layer_7_cast_fp16 = transpose(perm = var_344, x = x_43_cast_fp16)[name = string("transpose_45")];
            tensor<fp16, [?, 12, ?, 32]> context_layer_13_cast_fp16 = matmul(transpose_x = context_layer_13_transpose_x_0, transpose_y = context_layer_13_transpose_y_0, x = input_79_cast_fp16, y = value_layer_7_cast_fp16)[name = string("context_layer_13_cast_fp16")];
            tensor<int32, [4]> var_360 = const()[name = string("op_360"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [?, ?, 12, 32]> var_361_cast_fp16 = transpose(perm = var_360, x = context_layer_13_cast_fp16)[name = string("transpose_44")];
            tensor<int32, [4]> var_363_shape_cast_fp16 = shape(x = var_361_cast_fp16)[name = string("op_363_shape_cast_fp16")];
            int32 gather_31_axis_0 = const()[name = string("gather_31_axis_0"), val = int32(0)];
            int32 gather_31_batch_dims_0 = const()[name = string("gather_31_batch_dims_0"), val = int32(0)];
            bool gather_31_validate_indices_0 = const()[name = string("gather_31_validate_indices_0"), val = bool(false)];
            string var_363_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_363_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_31_indices_0_to_uint16 = const()[name = string("gather_31_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [4]> var_363_shape_cast_fp16_to_uint16 = cast(dtype = var_363_shape_cast_fp16_to_uint16_dtype_0, x = var_363_shape_cast_fp16)[name = string("cast_26")];
            uint16 gather_31_cast_uint16 = gather(axis = gather_31_axis_0, batch_dims = gather_31_batch_dims_0, indices = gather_31_indices_0_to_uint16, validate_indices = gather_31_validate_indices_0, x = var_363_shape_cast_fp16_to_uint16)[name = string("gather_31_cast_uint16")];
            string gather_31_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_31_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_32_axis_0 = const()[name = string("gather_32_axis_0"), val = int32(0)];
            int32 gather_32_batch_dims_0 = const()[name = string("gather_32_batch_dims_0"), val = int32(0)];
            bool gather_32_validate_indices_0 = const()[name = string("gather_32_validate_indices_0"), val = bool(false)];
            uint16 gather_32_indices_0_to_uint16 = const()[name = string("gather_32_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_32_cast_uint16 = gather(axis = gather_32_axis_0, batch_dims = gather_32_batch_dims_0, indices = gather_32_indices_0_to_uint16, validate_indices = gather_32_validate_indices_0, x = var_363_shape_cast_fp16_to_uint16)[name = string("gather_32_cast_uint16")];
            string gather_32_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_32_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_16_axis_0 = const()[name = string("concat_16_axis_0"), val = int32(0)];
            bool concat_16_interleave_0 = const()[name = string("concat_16_interleave_0"), val = bool(false)];
            int32 gather_32_cast_uint16_to_int32 = cast(dtype = gather_32_cast_uint16_to_int32_dtype_0, x = gather_32_cast_uint16)[name = string("cast_24")];
            int32 gather_31_cast_uint16_to_int32 = cast(dtype = gather_31_cast_uint16_to_int32_dtype_0, x = gather_31_cast_uint16)[name = string("cast_25")];
            tensor<int32, [3]> concat_16 = concat(axis = concat_16_axis_0, interleave = concat_16_interleave_0, values = (gather_31_cast_uint16_to_int32, gather_32_cast_uint16_to_int32, var_27))[name = string("concat_16")];
            tensor<fp16, [?, ?, 384]> input_81_cast_fp16 = reshape(shape = concat_16, x = var_361_cast_fp16)[name = string("input_81_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17795712))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17943232))))[name = string("model_encoder_layer_3_attention_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_3_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17944064)))];
            fp16 quantize_45_scale_0 = const()[name = string("quantize_45_scale_0"), val = fp16(nan)];
            string quantize_45_output_dtype_0 = const()[name = string("quantize_45_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_45 = quantize(input = input_81_cast_fp16, output_dtype = quantize_45_output_dtype_0, scale = quantize_45_scale_0)[name = string("quantize_45")];
            fp16 dequantize_45_scale_0 = const()[name = string("dequantize_45_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_45 = dequantize(input = quantize_45, scale = dequantize_45_scale_0)[name = string("dequantize_45")];
            tensor<fp16, [?, ?, 384]> linear_21_cast_fp16 = linear(bias = model_encoder_layer_3_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_3_attention_output_dense_weight_to_fp16_quantized, x = dequantize_45)[name = string("linear_21_cast_fp16")];
            fp16 quantize_46_scale_0 = const()[name = string("quantize_46_scale_0"), val = fp16(nan)];
            string quantize_46_output_dtype_0 = const()[name = string("quantize_46_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_46 = quantize(input = linear_21_cast_fp16, output_dtype = quantize_46_output_dtype_0, scale = quantize_46_scale_0)[name = string("quantize_46")];
            fp16 quantize_47_scale_0 = const()[name = string("quantize_47_scale_0"), val = fp16(nan)];
            string quantize_47_output_dtype_0 = const()[name = string("quantize_47_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_47 = quantize(input = input_75_cast_fp16, output_dtype = quantize_47_output_dtype_0, scale = quantize_47_scale_0)[name = string("quantize_47")];
            fp16 dequantize_190_scale_0 = const()[name = string("dequantize_190_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_190 = dequantize(input = quantize_47, scale = dequantize_190_scale_0)[name = string("dequantize_190")];
            fp16 dequantize_24_scale_0_1 = const()[name = string("dequantize_24_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_24_1 = dequantize(input = quantize_46, scale = dequantize_24_scale_0_1)[name = string("dequantize_24_1")];
            tensor<fp16, [?, ?, 384]> input_85_cast_fp16 = add(x = dequantize_24_1, y = dequantize_190)[name = string("input_85_cast_fp16")];
            string quantize_103_output_dtype_0 = const()[name = string("quantize_103_output_dtype_0"), val = string("int8")];
            fp16 quantize_12_scale_0_1 = const()[name = string("quantize_12_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_12_1 = quantize(input = input_85_cast_fp16, output_dtype = quantize_103_output_dtype_0, scale = quantize_12_scale_0_1)[name = string("quantize_12_1")];
            fp16 dequantize_103_scale_0 = const()[name = string("dequantize_103_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_147 = dequantize(input = quantize_12_1, scale = dequantize_103_scale_0)[name = string("dequantize_147")];
            tensor<int32, [1]> input_87_axes_0 = const()[name = string("input_87_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17944896)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17945728)))];
            tensor<fp16, [?, ?, 384]> input_87_cast_fp16 = layer_norm(axes = input_87_axes_0, beta = model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16, x = dequantize_147)[name = string("input_87_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_3_intermediate_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [1536, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(17946560))), scale = tensor<fp16, [1536, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(18536448))))[name = string("model_encoder_layer_3_intermediate_dense_weight_to_fp16_quantized")];
            tensor<fp16, [1536]> model_encoder_layer_3_intermediate_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_3_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(18539584)))];
            fp16 quantize_48_scale_0 = const()[name = string("quantize_48_scale_0"), val = fp16(nan)];
            string quantize_48_output_dtype_0 = const()[name = string("quantize_48_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_48 = quantize(input = input_87_cast_fp16, output_dtype = quantize_48_output_dtype_0, scale = quantize_48_scale_0)[name = string("quantize_48")];
            fp16 dequantize_48_scale_0 = const()[name = string("dequantize_48_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_48 = dequantize(input = quantize_48, scale = dequantize_48_scale_0)[name = string("dequantize_48")];
            tensor<fp16, [?, ?, 1536]> linear_22_cast_fp16 = linear(bias = model_encoder_layer_3_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_3_intermediate_dense_weight_to_fp16_quantized, x = dequantize_48)[name = string("linear_22_cast_fp16")];
            fp16 quantize_104_scale_0 = const()[name = string("quantize_104_scale_0"), val = fp16(nan)];
            string quantize_104_output_dtype_0 = const()[name = string("quantize_104_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_148 = quantize(input = linear_22_cast_fp16, output_dtype = quantize_104_output_dtype_0, scale = quantize_104_scale_0)[name = string("quantize_148")];
            fp16 dequantize_104_scale_0 = const()[name = string("dequantize_104_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_148 = dequantize(input = quantize_148, scale = dequantize_104_scale_0)[name = string("dequantize_148")];
            string input_91_mode_0 = const()[name = string("input_91_mode_0"), val = string("EXACT")];
            tensor<fp16, [?, ?, 1536]> input_91_cast_fp16 = gelu(mode = input_91_mode_0, x = dequantize_148)[name = string("input_91_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_3_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(18542720))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19132608))))[name = string("model_encoder_layer_3_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_3_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_3_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19133440)))];
            fp16 quantize_49_scale_0 = const()[name = string("quantize_49_scale_0"), val = fp16(nan)];
            string quantize_49_output_dtype_0 = const()[name = string("quantize_49_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_49 = quantize(input = input_91_cast_fp16, output_dtype = quantize_49_output_dtype_0, scale = quantize_49_scale_0)[name = string("quantize_49")];
            fp16 dequantize_49_scale_0 = const()[name = string("dequantize_49_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_49 = dequantize(input = quantize_49, scale = dequantize_49_scale_0)[name = string("dequantize_49")];
            tensor<fp16, [?, ?, 384]> linear_23_cast_fp16 = linear(bias = model_encoder_layer_3_output_dense_bias_to_fp16, weight = model_encoder_layer_3_output_dense_weight_to_fp16_quantized, x = dequantize_49)[name = string("linear_23_cast_fp16")];
            fp16 quantize_50_scale_0 = const()[name = string("quantize_50_scale_0"), val = fp16(nan)];
            string quantize_50_output_dtype_0 = const()[name = string("quantize_50_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_50 = quantize(input = linear_23_cast_fp16, output_dtype = quantize_50_output_dtype_0, scale = quantize_50_scale_0)[name = string("quantize_50")];
            fp16 quantize_51_scale_0 = const()[name = string("quantize_51_scale_0"), val = fp16(nan)];
            string quantize_51_output_dtype_0 = const()[name = string("quantize_51_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_51 = quantize(input = input_87_cast_fp16, output_dtype = quantize_51_output_dtype_0, scale = quantize_51_scale_0)[name = string("quantize_51")];
            fp16 dequantize_192_scale_0 = const()[name = string("dequantize_192_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_192 = dequantize(input = quantize_51, scale = dequantize_192_scale_0)[name = string("dequantize_192")];
            fp16 dequantize_26_scale_0 = const()[name = string("dequantize_26_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_26 = dequantize(input = quantize_50, scale = dequantize_26_scale_0)[name = string("dequantize_26")];
            tensor<fp16, [?, ?, 384]> input_95_cast_fp16 = add(x = dequantize_26, y = dequantize_192)[name = string("input_95_cast_fp16")];
            string quantize_105_output_dtype_0 = const()[name = string("quantize_105_output_dtype_0"), val = string("int8")];
            fp16 quantize_13_scale_0_1 = const()[name = string("quantize_13_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_13_1 = quantize(input = input_95_cast_fp16, output_dtype = quantize_105_output_dtype_0, scale = quantize_13_scale_0_1)[name = string("quantize_13_1")];
            fp16 dequantize_105_scale_0 = const()[name = string("dequantize_105_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_149 = dequantize(input = quantize_13_1, scale = dequantize_105_scale_0)[name = string("dequantize_149")];
            tensor<int32, [1]> input_97_axes_0 = const()[name = string("input_97_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_3_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19134272)))];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_3_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19135104)))];
            tensor<fp16, [?, ?, 384]> input_97_cast_fp16 = layer_norm(axes = input_97_axes_0, beta = model_encoder_layer_3_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_3_output_LayerNorm_weight_to_fp16, x = dequantize_149)[name = string("input_97_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_query_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19135936))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19283456))))[name = string("model_encoder_layer_4_attention_self_query_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_query_bias_to_fp16 = const()[name = string("model_encoder_layer_4_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19284288)))];
            fp16 quantize_52_scale_0 = const()[name = string("quantize_52_scale_0"), val = fp16(nan)];
            string quantize_52_output_dtype_0 = const()[name = string("quantize_52_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_52 = quantize(input = input_97_cast_fp16, output_dtype = quantize_52_output_dtype_0, scale = quantize_52_scale_0)[name = string("quantize_52")];
            fp16 dequantize_52_scale_0 = const()[name = string("dequantize_52_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_52 = dequantize(input = quantize_52, scale = dequantize_52_scale_0)[name = string("dequantize_52")];
            tensor<fp16, [?, ?, 384]> linear_24_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_query_bias_to_fp16, weight = model_encoder_layer_4_attention_self_query_weight_to_fp16_quantized, x = dequantize_52)[name = string("linear_24_cast_fp16")];
            fp16 quantize_106_scale_0 = const()[name = string("quantize_106_scale_0"), val = fp16(nan)];
            string quantize_106_output_dtype_0 = const()[name = string("quantize_106_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_150 = quantize(input = linear_24_cast_fp16, output_dtype = quantize_106_output_dtype_0, scale = quantize_106_scale_0)[name = string("quantize_150")];
            fp16 dequantize_106_scale_0 = const()[name = string("dequantize_106_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_150 = dequantize(input = quantize_150, scale = dequantize_106_scale_0)[name = string("dequantize_150")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_key_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19285120))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19432640))))[name = string("model_encoder_layer_4_attention_self_key_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_key_bias_to_fp16 = const()[name = string("model_encoder_layer_4_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19433472)))];
            fp16 quantize_53_scale_0 = const()[name = string("quantize_53_scale_0"), val = fp16(nan)];
            string quantize_53_output_dtype_0 = const()[name = string("quantize_53_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_53 = quantize(input = input_97_cast_fp16, output_dtype = quantize_53_output_dtype_0, scale = quantize_53_scale_0)[name = string("quantize_53")];
            fp16 dequantize_53_scale_0 = const()[name = string("dequantize_53_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_53 = dequantize(input = quantize_53, scale = dequantize_53_scale_0)[name = string("dequantize_53")];
            tensor<fp16, [?, ?, 384]> linear_25_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_key_bias_to_fp16, weight = model_encoder_layer_4_attention_self_key_weight_to_fp16_quantized, x = dequantize_53)[name = string("linear_25_cast_fp16")];
            fp16 quantize_107_scale_0 = const()[name = string("quantize_107_scale_0"), val = fp16(nan)];
            string quantize_107_output_dtype_0 = const()[name = string("quantize_107_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_151 = quantize(input = linear_25_cast_fp16, output_dtype = quantize_107_output_dtype_0, scale = quantize_107_scale_0)[name = string("quantize_151")];
            fp16 dequantize_107_scale_0 = const()[name = string("dequantize_107_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_151 = dequantize(input = quantize_151, scale = dequantize_107_scale_0)[name = string("dequantize_151")];
            tensor<int32, [3]> var_408_shape_cast_fp16 = shape(x = dequantize_151)[name = string("op_408_shape_cast_fp16")];
            int32 gather_33_axis_0 = const()[name = string("gather_33_axis_0"), val = int32(0)];
            int32 gather_33_batch_dims_0 = const()[name = string("gather_33_batch_dims_0"), val = int32(0)];
            bool gather_33_validate_indices_0 = const()[name = string("gather_33_validate_indices_0"), val = bool(false)];
            string var_408_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_408_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_33_indices_0_to_uint16 = const()[name = string("gather_33_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_408_shape_cast_fp16_to_uint16 = cast(dtype = var_408_shape_cast_fp16_to_uint16_dtype_0, x = var_408_shape_cast_fp16)[name = string("cast_23")];
            uint16 gather_33_cast_uint16 = gather(axis = gather_33_axis_0, batch_dims = gather_33_batch_dims_0, indices = gather_33_indices_0_to_uint16, validate_indices = gather_33_validate_indices_0, x = var_408_shape_cast_fp16_to_uint16)[name = string("gather_33_cast_uint16")];
            string gather_33_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_33_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_34_axis_0 = const()[name = string("gather_34_axis_0"), val = int32(0)];
            int32 gather_34_batch_dims_0 = const()[name = string("gather_34_batch_dims_0"), val = int32(0)];
            bool gather_34_validate_indices_0 = const()[name = string("gather_34_validate_indices_0"), val = bool(false)];
            uint16 gather_34_indices_0_to_uint16 = const()[name = string("gather_34_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_34_cast_uint16 = gather(axis = gather_34_axis_0, batch_dims = gather_34_batch_dims_0, indices = gather_34_indices_0_to_uint16, validate_indices = gather_34_validate_indices_0, x = var_408_shape_cast_fp16_to_uint16)[name = string("gather_34_cast_uint16")];
            string gather_34_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_34_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_17_axis_0 = const()[name = string("concat_17_axis_0"), val = int32(0)];
            bool concat_17_interleave_0 = const()[name = string("concat_17_interleave_0"), val = bool(false)];
            int32 gather_34_cast_uint16_to_int32 = cast(dtype = gather_34_cast_uint16_to_int32_dtype_0, x = gather_34_cast_uint16)[name = string("cast_21")];
            int32 gather_33_cast_uint16_to_int32 = cast(dtype = gather_33_cast_uint16_to_int32_dtype_0, x = gather_33_cast_uint16)[name = string("cast_22")];
            tensor<int32, [4]> concat_17 = concat(axis = concat_17_axis_0, interleave = concat_17_interleave_0, values = (gather_33_cast_uint16_to_int32, gather_34_cast_uint16_to_int32, var_23, var_22))[name = string("concat_17")];
            tensor<fp16, [?, ?, 12, 32]> x_51_cast_fp16 = reshape(shape = concat_17, x = dequantize_151)[name = string("x_51_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_value_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19434304))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19581824))))[name = string("model_encoder_layer_4_attention_self_value_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_value_bias_to_fp16 = const()[name = string("model_encoder_layer_4_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19582656)))];
            fp16 quantize_54_scale_0 = const()[name = string("quantize_54_scale_0"), val = fp16(nan)];
            string quantize_54_output_dtype_0 = const()[name = string("quantize_54_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_54 = quantize(input = input_97_cast_fp16, output_dtype = quantize_54_output_dtype_0, scale = quantize_54_scale_0)[name = string("quantize_54")];
            fp16 dequantize_54_scale_0 = const()[name = string("dequantize_54_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_54 = dequantize(input = quantize_54, scale = dequantize_54_scale_0)[name = string("dequantize_54")];
            tensor<fp16, [?, ?, 384]> linear_26_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_value_bias_to_fp16, weight = model_encoder_layer_4_attention_self_value_weight_to_fp16_quantized, x = dequantize_54)[name = string("linear_26_cast_fp16")];
            fp16 quantize_108_scale_0 = const()[name = string("quantize_108_scale_0"), val = fp16(nan)];
            string quantize_108_output_dtype_0 = const()[name = string("quantize_108_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_152 = quantize(input = linear_26_cast_fp16, output_dtype = quantize_108_output_dtype_0, scale = quantize_108_scale_0)[name = string("quantize_152")];
            fp16 dequantize_108_scale_0 = const()[name = string("dequantize_108_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_152 = dequantize(input = quantize_152, scale = dequantize_108_scale_0)[name = string("dequantize_152")];
            tensor<int32, [3]> var_417_shape_cast_fp16 = shape(x = dequantize_152)[name = string("op_417_shape_cast_fp16")];
            int32 gather_35_axis_0 = const()[name = string("gather_35_axis_0"), val = int32(0)];
            int32 gather_35_batch_dims_0 = const()[name = string("gather_35_batch_dims_0"), val = int32(0)];
            bool gather_35_validate_indices_0 = const()[name = string("gather_35_validate_indices_0"), val = bool(false)];
            string var_417_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_417_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_35_indices_0_to_uint16 = const()[name = string("gather_35_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_417_shape_cast_fp16_to_uint16 = cast(dtype = var_417_shape_cast_fp16_to_uint16_dtype_0, x = var_417_shape_cast_fp16)[name = string("cast_20")];
            uint16 gather_35_cast_uint16 = gather(axis = gather_35_axis_0, batch_dims = gather_35_batch_dims_0, indices = gather_35_indices_0_to_uint16, validate_indices = gather_35_validate_indices_0, x = var_417_shape_cast_fp16_to_uint16)[name = string("gather_35_cast_uint16")];
            string gather_35_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_35_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_36_axis_0 = const()[name = string("gather_36_axis_0"), val = int32(0)];
            int32 gather_36_batch_dims_0 = const()[name = string("gather_36_batch_dims_0"), val = int32(0)];
            bool gather_36_validate_indices_0 = const()[name = string("gather_36_validate_indices_0"), val = bool(false)];
            uint16 gather_36_indices_0_to_uint16 = const()[name = string("gather_36_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_36_cast_uint16 = gather(axis = gather_36_axis_0, batch_dims = gather_36_batch_dims_0, indices = gather_36_indices_0_to_uint16, validate_indices = gather_36_validate_indices_0, x = var_417_shape_cast_fp16_to_uint16)[name = string("gather_36_cast_uint16")];
            string gather_36_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_36_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_18_axis_0 = const()[name = string("concat_18_axis_0"), val = int32(0)];
            bool concat_18_interleave_0 = const()[name = string("concat_18_interleave_0"), val = bool(false)];
            int32 gather_36_cast_uint16_to_int32 = cast(dtype = gather_36_cast_uint16_to_int32_dtype_0, x = gather_36_cast_uint16)[name = string("cast_18")];
            int32 gather_35_cast_uint16_to_int32 = cast(dtype = gather_35_cast_uint16_to_int32_dtype_0, x = gather_35_cast_uint16)[name = string("cast_19")];
            tensor<int32, [4]> concat_18 = concat(axis = concat_18_axis_0, interleave = concat_18_interleave_0, values = (gather_35_cast_uint16_to_int32, gather_36_cast_uint16_to_int32, var_23, var_22))[name = string("concat_18")];
            tensor<fp16, [?, ?, 12, 32]> x_55_cast_fp16 = reshape(shape = concat_18, x = dequantize_152)[name = string("x_55_cast_fp16")];
            tensor<int32, [4]> var_421 = const()[name = string("op_421"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [3]> var_423_shape_cast_fp16 = shape(x = dequantize_150)[name = string("op_423_shape_cast_fp16")];
            int32 gather_37_axis_0 = const()[name = string("gather_37_axis_0"), val = int32(0)];
            int32 gather_37_batch_dims_0 = const()[name = string("gather_37_batch_dims_0"), val = int32(0)];
            bool gather_37_validate_indices_0 = const()[name = string("gather_37_validate_indices_0"), val = bool(false)];
            string var_423_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_423_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_37_indices_0_to_uint16 = const()[name = string("gather_37_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_423_shape_cast_fp16_to_uint16 = cast(dtype = var_423_shape_cast_fp16_to_uint16_dtype_0, x = var_423_shape_cast_fp16)[name = string("cast_17")];
            uint16 gather_37_cast_uint16 = gather(axis = gather_37_axis_0, batch_dims = gather_37_batch_dims_0, indices = gather_37_indices_0_to_uint16, validate_indices = gather_37_validate_indices_0, x = var_423_shape_cast_fp16_to_uint16)[name = string("gather_37_cast_uint16")];
            string gather_37_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_37_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_38_axis_0 = const()[name = string("gather_38_axis_0"), val = int32(0)];
            int32 gather_38_batch_dims_0 = const()[name = string("gather_38_batch_dims_0"), val = int32(0)];
            bool gather_38_validate_indices_0 = const()[name = string("gather_38_validate_indices_0"), val = bool(false)];
            uint16 gather_38_indices_0_to_uint16 = const()[name = string("gather_38_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_38_cast_uint16 = gather(axis = gather_38_axis_0, batch_dims = gather_38_batch_dims_0, indices = gather_38_indices_0_to_uint16, validate_indices = gather_38_validate_indices_0, x = var_423_shape_cast_fp16_to_uint16)[name = string("gather_38_cast_uint16")];
            string gather_38_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_38_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_19_axis_0 = const()[name = string("concat_19_axis_0"), val = int32(0)];
            bool concat_19_interleave_0 = const()[name = string("concat_19_interleave_0"), val = bool(false)];
            int32 gather_38_cast_uint16_to_int32 = cast(dtype = gather_38_cast_uint16_to_int32_dtype_0, x = gather_38_cast_uint16)[name = string("cast_15")];
            int32 gather_37_cast_uint16_to_int32 = cast(dtype = gather_37_cast_uint16_to_int32_dtype_0, x = gather_37_cast_uint16)[name = string("cast_16")];
            tensor<int32, [4]> concat_19 = concat(axis = concat_19_axis_0, interleave = concat_19_interleave_0, values = (gather_37_cast_uint16_to_int32, gather_38_cast_uint16_to_int32, var_23, var_22))[name = string("concat_19")];
            tensor<fp16, [?, ?, 12, 32]> x_59_cast_fp16 = reshape(shape = concat_19, x = dequantize_150)[name = string("x_59_cast_fp16")];
            bool attention_scores_17_transpose_x_0 = const()[name = string("attention_scores_17_transpose_x_0"), val = bool(false)];
            bool attention_scores_17_transpose_y_0 = const()[name = string("attention_scores_17_transpose_y_0"), val = bool(false)];
            tensor<int32, [4]> transpose_32_perm_0 = const()[name = string("transpose_32_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_33_perm_0 = const()[name = string("transpose_33_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [?, 12, 32, ?]> transpose_33 = transpose(perm = transpose_33_perm_0, x = x_51_cast_fp16)[name = string("transpose_42")];
            tensor<fp16, [?, 12, ?, 32]> transpose_32 = transpose(perm = transpose_32_perm_0, x = x_59_cast_fp16)[name = string("transpose_43")];
            tensor<fp16, [?, 12, ?, ?]> attention_scores_17_cast_fp16 = matmul(transpose_x = attention_scores_17_transpose_x_0, transpose_y = attention_scores_17_transpose_y_0, x = transpose_32, y = transpose_33)[name = string("attention_scores_17_cast_fp16")];
            fp16 _inversed_attention_scores_19_y_0_to_fp16 = const()[name = string("_inversed_attention_scores_19_y_0_to_fp16"), val = fp16(0x1.6ap-3)];
            tensor<fp16, [?, 12, ?, ?]> _inversed_attention_scores_19_cast_fp16 = mul(x = attention_scores_17_cast_fp16, y = _inversed_attention_scores_19_y_0_to_fp16)[name = string("_inversed_attention_scores_19_cast_fp16")];
            fp16 quantize_55_scale_0 = const()[name = string("quantize_55_scale_0"), val = fp16(nan)];
            string quantize_55_output_dtype_0 = const()[name = string("quantize_55_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 12, ?, ?]> quantize_55 = quantize(input = _inversed_attention_scores_19_cast_fp16, output_dtype = quantize_55_output_dtype_0, scale = quantize_55_scale_0)[name = string("quantize_55")];
            fp16 quantize_56_scale_0 = const()[name = string("quantize_56_scale_0"), val = fp16(nan)];
            string quantize_56_output_dtype_0 = const()[name = string("quantize_56_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 1, 1, ?]> quantize_56 = quantize(input = attention_mask_cast_fp16, output_dtype = quantize_56_output_dtype_0, scale = quantize_56_scale_0)[name = string("quantize_56")];
            fp16 dequantize_194_scale_0 = const()[name = string("dequantize_194_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, 1, 1, ?]> dequantize_194 = dequantize(input = quantize_56, scale = dequantize_194_scale_0)[name = string("dequantize_194")];
            fp16 dequantize_28_scale_0_1 = const()[name = string("dequantize_28_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_28_1 = dequantize(input = quantize_55, scale = dequantize_28_scale_0_1)[name = string("dequantize_28_1")];
            tensor<fp16, [?, 12, ?, ?]> input_99_cast_fp16 = add(x = dequantize_28_1, y = dequantize_194)[name = string("input_99_cast_fp16")];
            string quantize_109_output_dtype_0 = const()[name = string("quantize_109_output_dtype_0"), val = string("int8")];
            fp16 quantize_14_scale_0_1 = const()[name = string("quantize_14_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, 12, ?, ?]> quantize_14_1 = quantize(input = input_99_cast_fp16, output_dtype = quantize_109_output_dtype_0, scale = quantize_14_scale_0_1)[name = string("quantize_14_1")];
            fp16 dequantize_109_scale_0 = const()[name = string("dequantize_109_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_153 = dequantize(input = quantize_14_1, scale = dequantize_109_scale_0)[name = string("dequantize_153")];
            tensor<fp16, [?, 12, ?, ?]> input_101_cast_fp16 = softmax(axis = var_24, x = dequantize_153)[name = string("input_101_cast_fp16")];
            bool context_layer_17_transpose_x_0 = const()[name = string("context_layer_17_transpose_x_0"), val = bool(false)];
            bool context_layer_17_transpose_y_0 = const()[name = string("context_layer_17_transpose_y_0"), val = bool(false)];
            tensor<fp16, [?, 12, ?, 32]> value_layer_9_cast_fp16 = transpose(perm = var_421, x = x_55_cast_fp16)[name = string("transpose_41")];
            tensor<fp16, [?, 12, ?, 32]> context_layer_17_cast_fp16 = matmul(transpose_x = context_layer_17_transpose_x_0, transpose_y = context_layer_17_transpose_y_0, x = input_101_cast_fp16, y = value_layer_9_cast_fp16)[name = string("context_layer_17_cast_fp16")];
            tensor<int32, [4]> var_437 = const()[name = string("op_437"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [?, ?, 12, 32]> var_438_cast_fp16 = transpose(perm = var_437, x = context_layer_17_cast_fp16)[name = string("transpose_40")];
            tensor<int32, [4]> var_440_shape_cast_fp16 = shape(x = var_438_cast_fp16)[name = string("op_440_shape_cast_fp16")];
            int32 gather_39_axis_0 = const()[name = string("gather_39_axis_0"), val = int32(0)];
            int32 gather_39_batch_dims_0 = const()[name = string("gather_39_batch_dims_0"), val = int32(0)];
            bool gather_39_validate_indices_0 = const()[name = string("gather_39_validate_indices_0"), val = bool(false)];
            string var_440_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_440_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_39_indices_0_to_uint16 = const()[name = string("gather_39_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [4]> var_440_shape_cast_fp16_to_uint16 = cast(dtype = var_440_shape_cast_fp16_to_uint16_dtype_0, x = var_440_shape_cast_fp16)[name = string("cast_14")];
            uint16 gather_39_cast_uint16 = gather(axis = gather_39_axis_0, batch_dims = gather_39_batch_dims_0, indices = gather_39_indices_0_to_uint16, validate_indices = gather_39_validate_indices_0, x = var_440_shape_cast_fp16_to_uint16)[name = string("gather_39_cast_uint16")];
            string gather_39_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_39_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_40_axis_0 = const()[name = string("gather_40_axis_0"), val = int32(0)];
            int32 gather_40_batch_dims_0 = const()[name = string("gather_40_batch_dims_0"), val = int32(0)];
            bool gather_40_validate_indices_0 = const()[name = string("gather_40_validate_indices_0"), val = bool(false)];
            uint16 gather_40_indices_0_to_uint16 = const()[name = string("gather_40_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_40_cast_uint16 = gather(axis = gather_40_axis_0, batch_dims = gather_40_batch_dims_0, indices = gather_40_indices_0_to_uint16, validate_indices = gather_40_validate_indices_0, x = var_440_shape_cast_fp16_to_uint16)[name = string("gather_40_cast_uint16")];
            string gather_40_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_40_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_20_axis_0 = const()[name = string("concat_20_axis_0"), val = int32(0)];
            bool concat_20_interleave_0 = const()[name = string("concat_20_interleave_0"), val = bool(false)];
            int32 gather_40_cast_uint16_to_int32 = cast(dtype = gather_40_cast_uint16_to_int32_dtype_0, x = gather_40_cast_uint16)[name = string("cast_12")];
            int32 gather_39_cast_uint16_to_int32 = cast(dtype = gather_39_cast_uint16_to_int32_dtype_0, x = gather_39_cast_uint16)[name = string("cast_13")];
            tensor<int32, [3]> concat_20 = concat(axis = concat_20_axis_0, interleave = concat_20_interleave_0, values = (gather_39_cast_uint16_to_int32, gather_40_cast_uint16_to_int32, var_27))[name = string("concat_20")];
            tensor<fp16, [?, ?, 384]> input_103_cast_fp16 = reshape(shape = concat_20, x = var_438_cast_fp16)[name = string("input_103_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19583488))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19731008))))[name = string("model_encoder_layer_4_attention_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_4_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19731840)))];
            fp16 quantize_57_scale_0 = const()[name = string("quantize_57_scale_0"), val = fp16(nan)];
            string quantize_57_output_dtype_0 = const()[name = string("quantize_57_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_57 = quantize(input = input_103_cast_fp16, output_dtype = quantize_57_output_dtype_0, scale = quantize_57_scale_0)[name = string("quantize_57")];
            fp16 dequantize_57_scale_0 = const()[name = string("dequantize_57_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_57 = dequantize(input = quantize_57, scale = dequantize_57_scale_0)[name = string("dequantize_57")];
            tensor<fp16, [?, ?, 384]> linear_27_cast_fp16 = linear(bias = model_encoder_layer_4_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_4_attention_output_dense_weight_to_fp16_quantized, x = dequantize_57)[name = string("linear_27_cast_fp16")];
            fp16 quantize_58_scale_0 = const()[name = string("quantize_58_scale_0"), val = fp16(nan)];
            string quantize_58_output_dtype_0 = const()[name = string("quantize_58_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_58 = quantize(input = linear_27_cast_fp16, output_dtype = quantize_58_output_dtype_0, scale = quantize_58_scale_0)[name = string("quantize_58")];
            fp16 quantize_59_scale_0 = const()[name = string("quantize_59_scale_0"), val = fp16(nan)];
            string quantize_59_output_dtype_0 = const()[name = string("quantize_59_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_59 = quantize(input = input_97_cast_fp16, output_dtype = quantize_59_output_dtype_0, scale = quantize_59_scale_0)[name = string("quantize_59")];
            fp16 dequantize_196_scale_0 = const()[name = string("dequantize_196_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_196 = dequantize(input = quantize_59, scale = dequantize_196_scale_0)[name = string("dequantize_196")];
            fp16 dequantize_30_scale_0_1 = const()[name = string("dequantize_30_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_30_1 = dequantize(input = quantize_58, scale = dequantize_30_scale_0_1)[name = string("dequantize_30_1")];
            tensor<fp16, [?, ?, 384]> input_107_cast_fp16 = add(x = dequantize_30_1, y = dequantize_196)[name = string("input_107_cast_fp16")];
            string quantize_110_output_dtype_0 = const()[name = string("quantize_110_output_dtype_0"), val = string("int8")];
            fp16 quantize_15_scale_0_1 = const()[name = string("quantize_15_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_15_1 = quantize(input = input_107_cast_fp16, output_dtype = quantize_110_output_dtype_0, scale = quantize_15_scale_0_1)[name = string("quantize_15_1")];
            fp16 dequantize_110_scale_0 = const()[name = string("dequantize_110_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_154 = dequantize(input = quantize_15_1, scale = dequantize_110_scale_0)[name = string("dequantize_154")];
            tensor<int32, [1]> input_109_axes_0 = const()[name = string("input_109_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19732672)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19733504)))];
            tensor<fp16, [?, ?, 384]> input_109_cast_fp16 = layer_norm(axes = input_109_axes_0, beta = model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16, x = dequantize_154)[name = string("input_109_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_4_intermediate_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [1536, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(19734336))), scale = tensor<fp16, [1536, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20324224))))[name = string("model_encoder_layer_4_intermediate_dense_weight_to_fp16_quantized")];
            tensor<fp16, [1536]> model_encoder_layer_4_intermediate_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_4_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20327360)))];
            fp16 quantize_60_scale_0 = const()[name = string("quantize_60_scale_0"), val = fp16(nan)];
            string quantize_60_output_dtype_0 = const()[name = string("quantize_60_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_60 = quantize(input = input_109_cast_fp16, output_dtype = quantize_60_output_dtype_0, scale = quantize_60_scale_0)[name = string("quantize_60")];
            fp16 dequantize_60_scale_0 = const()[name = string("dequantize_60_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_60 = dequantize(input = quantize_60, scale = dequantize_60_scale_0)[name = string("dequantize_60")];
            tensor<fp16, [?, ?, 1536]> linear_28_cast_fp16 = linear(bias = model_encoder_layer_4_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_4_intermediate_dense_weight_to_fp16_quantized, x = dequantize_60)[name = string("linear_28_cast_fp16")];
            fp16 quantize_111_scale_0 = const()[name = string("quantize_111_scale_0"), val = fp16(nan)];
            string quantize_111_output_dtype_0 = const()[name = string("quantize_111_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_155 = quantize(input = linear_28_cast_fp16, output_dtype = quantize_111_output_dtype_0, scale = quantize_111_scale_0)[name = string("quantize_155")];
            fp16 dequantize_111_scale_0 = const()[name = string("dequantize_111_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_155 = dequantize(input = quantize_155, scale = dequantize_111_scale_0)[name = string("dequantize_155")];
            string input_113_mode_0 = const()[name = string("input_113_mode_0"), val = string("EXACT")];
            tensor<fp16, [?, ?, 1536]> input_113_cast_fp16 = gelu(mode = input_113_mode_0, x = dequantize_155)[name = string("input_113_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_4_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20330496))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20920384))))[name = string("model_encoder_layer_4_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_4_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_4_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20921216)))];
            fp16 quantize_61_scale_0 = const()[name = string("quantize_61_scale_0"), val = fp16(nan)];
            string quantize_61_output_dtype_0 = const()[name = string("quantize_61_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_61 = quantize(input = input_113_cast_fp16, output_dtype = quantize_61_output_dtype_0, scale = quantize_61_scale_0)[name = string("quantize_61")];
            fp16 dequantize_61_scale_0 = const()[name = string("dequantize_61_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_61 = dequantize(input = quantize_61, scale = dequantize_61_scale_0)[name = string("dequantize_61")];
            tensor<fp16, [?, ?, 384]> linear_29_cast_fp16 = linear(bias = model_encoder_layer_4_output_dense_bias_to_fp16, weight = model_encoder_layer_4_output_dense_weight_to_fp16_quantized, x = dequantize_61)[name = string("linear_29_cast_fp16")];
            fp16 quantize_62_scale_0 = const()[name = string("quantize_62_scale_0"), val = fp16(nan)];
            string quantize_62_output_dtype_0 = const()[name = string("quantize_62_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_62 = quantize(input = linear_29_cast_fp16, output_dtype = quantize_62_output_dtype_0, scale = quantize_62_scale_0)[name = string("quantize_62")];
            fp16 quantize_63_scale_0 = const()[name = string("quantize_63_scale_0"), val = fp16(nan)];
            string quantize_63_output_dtype_0 = const()[name = string("quantize_63_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_63 = quantize(input = input_109_cast_fp16, output_dtype = quantize_63_output_dtype_0, scale = quantize_63_scale_0)[name = string("quantize_63")];
            fp16 dequantize_198_scale_0 = const()[name = string("dequantize_198_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_198 = dequantize(input = quantize_63, scale = dequantize_198_scale_0)[name = string("dequantize_198")];
            fp16 dequantize_32_scale_0 = const()[name = string("dequantize_32_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_32 = dequantize(input = quantize_62, scale = dequantize_32_scale_0)[name = string("dequantize_32")];
            tensor<fp16, [?, ?, 384]> input_117_cast_fp16 = add(x = dequantize_32, y = dequantize_198)[name = string("input_117_cast_fp16")];
            string quantize_112_output_dtype_0 = const()[name = string("quantize_112_output_dtype_0"), val = string("int8")];
            fp16 quantize_16_scale_0_1 = const()[name = string("quantize_16_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_16_1 = quantize(input = input_117_cast_fp16, output_dtype = quantize_112_output_dtype_0, scale = quantize_16_scale_0_1)[name = string("quantize_16_1")];
            fp16 dequantize_112_scale_0 = const()[name = string("dequantize_112_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_156 = dequantize(input = quantize_16_1, scale = dequantize_112_scale_0)[name = string("dequantize_156")];
            tensor<int32, [1]> input_119_axes_0 = const()[name = string("input_119_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_4_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20922048)))];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_4_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20922880)))];
            tensor<fp16, [?, ?, 384]> input_119_cast_fp16 = layer_norm(axes = input_119_axes_0, beta = model_encoder_layer_4_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_4_output_LayerNorm_weight_to_fp16, x = dequantize_156)[name = string("input_119_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_query_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(20923712))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21071232))))[name = string("model_encoder_layer_5_attention_self_query_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_query_bias_to_fp16 = const()[name = string("model_encoder_layer_5_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21072064)))];
            fp16 quantize_64_scale_0 = const()[name = string("quantize_64_scale_0"), val = fp16(nan)];
            string quantize_64_output_dtype_0 = const()[name = string("quantize_64_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_64 = quantize(input = input_119_cast_fp16, output_dtype = quantize_64_output_dtype_0, scale = quantize_64_scale_0)[name = string("quantize_64")];
            fp16 dequantize_64_scale_0 = const()[name = string("dequantize_64_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_64 = dequantize(input = quantize_64, scale = dequantize_64_scale_0)[name = string("dequantize_64")];
            tensor<fp16, [?, ?, 384]> linear_30_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_query_bias_to_fp16, weight = model_encoder_layer_5_attention_self_query_weight_to_fp16_quantized, x = dequantize_64)[name = string("linear_30_cast_fp16")];
            fp16 quantize_113_scale_0 = const()[name = string("quantize_113_scale_0"), val = fp16(nan)];
            string quantize_113_output_dtype_0 = const()[name = string("quantize_113_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_157 = quantize(input = linear_30_cast_fp16, output_dtype = quantize_113_output_dtype_0, scale = quantize_113_scale_0)[name = string("quantize_157")];
            fp16 dequantize_113_scale_0 = const()[name = string("dequantize_113_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_157 = dequantize(input = quantize_157, scale = dequantize_113_scale_0)[name = string("dequantize_157")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_key_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21072896))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21220416))))[name = string("model_encoder_layer_5_attention_self_key_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_key_bias_to_fp16 = const()[name = string("model_encoder_layer_5_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21221248)))];
            fp16 quantize_65_scale_0 = const()[name = string("quantize_65_scale_0"), val = fp16(nan)];
            string quantize_65_output_dtype_0 = const()[name = string("quantize_65_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_65 = quantize(input = input_119_cast_fp16, output_dtype = quantize_65_output_dtype_0, scale = quantize_65_scale_0)[name = string("quantize_65")];
            fp16 dequantize_65_scale_0 = const()[name = string("dequantize_65_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_65 = dequantize(input = quantize_65, scale = dequantize_65_scale_0)[name = string("dequantize_65")];
            tensor<fp16, [?, ?, 384]> linear_31_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_key_bias_to_fp16, weight = model_encoder_layer_5_attention_self_key_weight_to_fp16_quantized, x = dequantize_65)[name = string("linear_31_cast_fp16")];
            fp16 quantize_114_scale_0 = const()[name = string("quantize_114_scale_0"), val = fp16(nan)];
            string quantize_114_output_dtype_0 = const()[name = string("quantize_114_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_158 = quantize(input = linear_31_cast_fp16, output_dtype = quantize_114_output_dtype_0, scale = quantize_114_scale_0)[name = string("quantize_158")];
            fp16 dequantize_114_scale_0 = const()[name = string("dequantize_114_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_158 = dequantize(input = quantize_158, scale = dequantize_114_scale_0)[name = string("dequantize_158")];
            tensor<int32, [3]> var_485_shape_cast_fp16 = shape(x = dequantize_158)[name = string("op_485_shape_cast_fp16")];
            int32 gather_41_axis_0 = const()[name = string("gather_41_axis_0"), val = int32(0)];
            int32 gather_41_batch_dims_0 = const()[name = string("gather_41_batch_dims_0"), val = int32(0)];
            bool gather_41_validate_indices_0 = const()[name = string("gather_41_validate_indices_0"), val = bool(false)];
            string var_485_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_485_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_41_indices_0_to_uint16 = const()[name = string("gather_41_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_485_shape_cast_fp16_to_uint16 = cast(dtype = var_485_shape_cast_fp16_to_uint16_dtype_0, x = var_485_shape_cast_fp16)[name = string("cast_11")];
            uint16 gather_41_cast_uint16 = gather(axis = gather_41_axis_0, batch_dims = gather_41_batch_dims_0, indices = gather_41_indices_0_to_uint16, validate_indices = gather_41_validate_indices_0, x = var_485_shape_cast_fp16_to_uint16)[name = string("gather_41_cast_uint16")];
            string gather_41_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_41_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_42_axis_0 = const()[name = string("gather_42_axis_0"), val = int32(0)];
            int32 gather_42_batch_dims_0 = const()[name = string("gather_42_batch_dims_0"), val = int32(0)];
            bool gather_42_validate_indices_0 = const()[name = string("gather_42_validate_indices_0"), val = bool(false)];
            uint16 gather_42_indices_0_to_uint16 = const()[name = string("gather_42_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_42_cast_uint16 = gather(axis = gather_42_axis_0, batch_dims = gather_42_batch_dims_0, indices = gather_42_indices_0_to_uint16, validate_indices = gather_42_validate_indices_0, x = var_485_shape_cast_fp16_to_uint16)[name = string("gather_42_cast_uint16")];
            string gather_42_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_42_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_21_axis_0 = const()[name = string("concat_21_axis_0"), val = int32(0)];
            bool concat_21_interleave_0 = const()[name = string("concat_21_interleave_0"), val = bool(false)];
            int32 gather_42_cast_uint16_to_int32 = cast(dtype = gather_42_cast_uint16_to_int32_dtype_0, x = gather_42_cast_uint16)[name = string("cast_9")];
            int32 gather_41_cast_uint16_to_int32 = cast(dtype = gather_41_cast_uint16_to_int32_dtype_0, x = gather_41_cast_uint16)[name = string("cast_10")];
            tensor<int32, [4]> concat_21 = concat(axis = concat_21_axis_0, interleave = concat_21_interleave_0, values = (gather_41_cast_uint16_to_int32, gather_42_cast_uint16_to_int32, var_23, var_22))[name = string("concat_21")];
            tensor<fp16, [?, ?, 12, 32]> x_63_cast_fp16 = reshape(shape = concat_21, x = dequantize_158)[name = string("x_63_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_value_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21222080))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21369600))))[name = string("model_encoder_layer_5_attention_self_value_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_value_bias_to_fp16 = const()[name = string("model_encoder_layer_5_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21370432)))];
            fp16 quantize_66_scale_0 = const()[name = string("quantize_66_scale_0"), val = fp16(nan)];
            string quantize_66_output_dtype_0 = const()[name = string("quantize_66_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_66 = quantize(input = input_119_cast_fp16, output_dtype = quantize_66_output_dtype_0, scale = quantize_66_scale_0)[name = string("quantize_66")];
            fp16 dequantize_66_scale_0 = const()[name = string("dequantize_66_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_66 = dequantize(input = quantize_66, scale = dequantize_66_scale_0)[name = string("dequantize_66")];
            tensor<fp16, [?, ?, 384]> linear_32_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_value_bias_to_fp16, weight = model_encoder_layer_5_attention_self_value_weight_to_fp16_quantized, x = dequantize_66)[name = string("linear_32_cast_fp16")];
            fp16 quantize_115_scale_0 = const()[name = string("quantize_115_scale_0"), val = fp16(nan)];
            string quantize_115_output_dtype_0 = const()[name = string("quantize_115_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_159 = quantize(input = linear_32_cast_fp16, output_dtype = quantize_115_output_dtype_0, scale = quantize_115_scale_0)[name = string("quantize_159")];
            fp16 dequantize_115_scale_0 = const()[name = string("dequantize_115_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_159 = dequantize(input = quantize_159, scale = dequantize_115_scale_0)[name = string("dequantize_159")];
            tensor<int32, [3]> var_494_shape_cast_fp16 = shape(x = dequantize_159)[name = string("op_494_shape_cast_fp16")];
            int32 gather_43_axis_0 = const()[name = string("gather_43_axis_0"), val = int32(0)];
            int32 gather_43_batch_dims_0 = const()[name = string("gather_43_batch_dims_0"), val = int32(0)];
            bool gather_43_validate_indices_0 = const()[name = string("gather_43_validate_indices_0"), val = bool(false)];
            string var_494_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_494_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_43_indices_0_to_uint16 = const()[name = string("gather_43_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_494_shape_cast_fp16_to_uint16 = cast(dtype = var_494_shape_cast_fp16_to_uint16_dtype_0, x = var_494_shape_cast_fp16)[name = string("cast_8")];
            uint16 gather_43_cast_uint16 = gather(axis = gather_43_axis_0, batch_dims = gather_43_batch_dims_0, indices = gather_43_indices_0_to_uint16, validate_indices = gather_43_validate_indices_0, x = var_494_shape_cast_fp16_to_uint16)[name = string("gather_43_cast_uint16")];
            string gather_43_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_43_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_44_axis_0 = const()[name = string("gather_44_axis_0"), val = int32(0)];
            int32 gather_44_batch_dims_0 = const()[name = string("gather_44_batch_dims_0"), val = int32(0)];
            bool gather_44_validate_indices_0 = const()[name = string("gather_44_validate_indices_0"), val = bool(false)];
            uint16 gather_44_indices_0_to_uint16 = const()[name = string("gather_44_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_44_cast_uint16 = gather(axis = gather_44_axis_0, batch_dims = gather_44_batch_dims_0, indices = gather_44_indices_0_to_uint16, validate_indices = gather_44_validate_indices_0, x = var_494_shape_cast_fp16_to_uint16)[name = string("gather_44_cast_uint16")];
            string gather_44_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_44_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_22_axis_0 = const()[name = string("concat_22_axis_0"), val = int32(0)];
            bool concat_22_interleave_0 = const()[name = string("concat_22_interleave_0"), val = bool(false)];
            int32 gather_44_cast_uint16_to_int32 = cast(dtype = gather_44_cast_uint16_to_int32_dtype_0, x = gather_44_cast_uint16)[name = string("cast_6")];
            int32 gather_43_cast_uint16_to_int32 = cast(dtype = gather_43_cast_uint16_to_int32_dtype_0, x = gather_43_cast_uint16)[name = string("cast_7")];
            tensor<int32, [4]> concat_22 = concat(axis = concat_22_axis_0, interleave = concat_22_interleave_0, values = (gather_43_cast_uint16_to_int32, gather_44_cast_uint16_to_int32, var_23, var_22))[name = string("concat_22")];
            tensor<fp16, [?, ?, 12, 32]> x_67_cast_fp16 = reshape(shape = concat_22, x = dequantize_159)[name = string("x_67_cast_fp16")];
            tensor<int32, [4]> var_498 = const()[name = string("op_498"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [3]> var_500_shape_cast_fp16 = shape(x = dequantize_157)[name = string("op_500_shape_cast_fp16")];
            int32 gather_45_axis_0 = const()[name = string("gather_45_axis_0"), val = int32(0)];
            int32 gather_45_batch_dims_0 = const()[name = string("gather_45_batch_dims_0"), val = int32(0)];
            bool gather_45_validate_indices_0 = const()[name = string("gather_45_validate_indices_0"), val = bool(false)];
            string var_500_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_500_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_45_indices_0_to_uint16 = const()[name = string("gather_45_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [3]> var_500_shape_cast_fp16_to_uint16 = cast(dtype = var_500_shape_cast_fp16_to_uint16_dtype_0, x = var_500_shape_cast_fp16)[name = string("cast_5")];
            uint16 gather_45_cast_uint16 = gather(axis = gather_45_axis_0, batch_dims = gather_45_batch_dims_0, indices = gather_45_indices_0_to_uint16, validate_indices = gather_45_validate_indices_0, x = var_500_shape_cast_fp16_to_uint16)[name = string("gather_45_cast_uint16")];
            string gather_45_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_45_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_46_axis_0 = const()[name = string("gather_46_axis_0"), val = int32(0)];
            int32 gather_46_batch_dims_0 = const()[name = string("gather_46_batch_dims_0"), val = int32(0)];
            bool gather_46_validate_indices_0 = const()[name = string("gather_46_validate_indices_0"), val = bool(false)];
            uint16 gather_46_indices_0_to_uint16 = const()[name = string("gather_46_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_46_cast_uint16 = gather(axis = gather_46_axis_0, batch_dims = gather_46_batch_dims_0, indices = gather_46_indices_0_to_uint16, validate_indices = gather_46_validate_indices_0, x = var_500_shape_cast_fp16_to_uint16)[name = string("gather_46_cast_uint16")];
            string gather_46_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_46_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_23_axis_0 = const()[name = string("concat_23_axis_0"), val = int32(0)];
            bool concat_23_interleave_0 = const()[name = string("concat_23_interleave_0"), val = bool(false)];
            int32 gather_46_cast_uint16_to_int32 = cast(dtype = gather_46_cast_uint16_to_int32_dtype_0, x = gather_46_cast_uint16)[name = string("cast_3")];
            int32 gather_45_cast_uint16_to_int32 = cast(dtype = gather_45_cast_uint16_to_int32_dtype_0, x = gather_45_cast_uint16)[name = string("cast_4")];
            tensor<int32, [4]> concat_23 = concat(axis = concat_23_axis_0, interleave = concat_23_interleave_0, values = (gather_45_cast_uint16_to_int32, gather_46_cast_uint16_to_int32, var_23, var_22))[name = string("concat_23")];
            tensor<fp16, [?, ?, 12, 32]> x_cast_fp16 = reshape(shape = concat_23, x = dequantize_157)[name = string("x_cast_fp16")];
            bool attention_scores_21_transpose_x_0 = const()[name = string("attention_scores_21_transpose_x_0"), val = bool(false)];
            bool attention_scores_21_transpose_y_0 = const()[name = string("attention_scores_21_transpose_y_0"), val = bool(false)];
            tensor<int32, [4]> transpose_34_perm_0 = const()[name = string("transpose_34_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_35_perm_0 = const()[name = string("transpose_35_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [?, 12, 32, ?]> transpose_35 = transpose(perm = transpose_35_perm_0, x = x_63_cast_fp16)[name = string("transpose_38")];
            tensor<fp16, [?, 12, ?, 32]> transpose_34 = transpose(perm = transpose_34_perm_0, x = x_cast_fp16)[name = string("transpose_39")];
            tensor<fp16, [?, 12, ?, ?]> attention_scores_21_cast_fp16 = matmul(transpose_x = attention_scores_21_transpose_x_0, transpose_y = attention_scores_21_transpose_y_0, x = transpose_34, y = transpose_35)[name = string("attention_scores_21_cast_fp16")];
            fp16 _inversed_attention_scores_y_0_to_fp16 = const()[name = string("_inversed_attention_scores_y_0_to_fp16"), val = fp16(0x1.6ap-3)];
            tensor<fp16, [?, 12, ?, ?]> _inversed_attention_scores_cast_fp16 = mul(x = attention_scores_21_cast_fp16, y = _inversed_attention_scores_y_0_to_fp16)[name = string("_inversed_attention_scores_cast_fp16")];
            fp16 quantize_67_scale_0 = const()[name = string("quantize_67_scale_0"), val = fp16(nan)];
            string quantize_67_output_dtype_0 = const()[name = string("quantize_67_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 12, ?, ?]> quantize_67 = quantize(input = _inversed_attention_scores_cast_fp16, output_dtype = quantize_67_output_dtype_0, scale = quantize_67_scale_0)[name = string("quantize_67")];
            fp16 quantize_68_scale_0 = const()[name = string("quantize_68_scale_0"), val = fp16(nan)];
            string quantize_68_output_dtype_0 = const()[name = string("quantize_68_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 1, 1, ?]> quantize_68 = quantize(input = attention_mask_cast_fp16, output_dtype = quantize_68_output_dtype_0, scale = quantize_68_scale_0)[name = string("quantize_68")];
            fp16 dequantize_200_scale_0 = const()[name = string("dequantize_200_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, 1, 1, ?]> dequantize_200 = dequantize(input = quantize_68, scale = dequantize_200_scale_0)[name = string("dequantize_200")];
            fp16 dequantize_34_scale_0 = const()[name = string("dequantize_34_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_34 = dequantize(input = quantize_67, scale = dequantize_34_scale_0)[name = string("dequantize_34")];
            tensor<fp16, [?, 12, ?, ?]> input_121_cast_fp16 = add(x = dequantize_34, y = dequantize_200)[name = string("input_121_cast_fp16")];
            string quantize_116_output_dtype_0 = const()[name = string("quantize_116_output_dtype_0"), val = string("int8")];
            fp16 quantize_17_scale_0_1 = const()[name = string("quantize_17_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, 12, ?, ?]> quantize_17_1 = quantize(input = input_121_cast_fp16, output_dtype = quantize_116_output_dtype_0, scale = quantize_17_scale_0_1)[name = string("quantize_17_1")];
            fp16 dequantize_116_scale_0 = const()[name = string("dequantize_116_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 12, ?, ?]> dequantize_160 = dequantize(input = quantize_17_1, scale = dequantize_116_scale_0)[name = string("dequantize_160")];
            tensor<fp16, [?, 12, ?, ?]> input_123_cast_fp16 = softmax(axis = var_24, x = dequantize_160)[name = string("input_123_cast_fp16")];
            bool context_layer_21_transpose_x_0 = const()[name = string("context_layer_21_transpose_x_0"), val = bool(false)];
            bool context_layer_21_transpose_y_0 = const()[name = string("context_layer_21_transpose_y_0"), val = bool(false)];
            tensor<fp16, [?, 12, ?, 32]> value_layer_cast_fp16 = transpose(perm = var_498, x = x_67_cast_fp16)[name = string("transpose_37")];
            tensor<fp16, [?, 12, ?, 32]> context_layer_21_cast_fp16 = matmul(transpose_x = context_layer_21_transpose_x_0, transpose_y = context_layer_21_transpose_y_0, x = input_123_cast_fp16, y = value_layer_cast_fp16)[name = string("context_layer_21_cast_fp16")];
            tensor<int32, [4]> var_514 = const()[name = string("op_514"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, [?, ?, 12, 32]> var_515_cast_fp16 = transpose(perm = var_514, x = context_layer_21_cast_fp16)[name = string("transpose_36")];
            tensor<int32, [4]> var_517_shape_cast_fp16 = shape(x = var_515_cast_fp16)[name = string("op_517_shape_cast_fp16")];
            int32 gather_47_axis_0 = const()[name = string("gather_47_axis_0"), val = int32(0)];
            int32 gather_47_batch_dims_0 = const()[name = string("gather_47_batch_dims_0"), val = int32(0)];
            bool gather_47_validate_indices_0 = const()[name = string("gather_47_validate_indices_0"), val = bool(false)];
            string var_517_shape_cast_fp16_to_uint16_dtype_0 = const()[name = string("op_517_shape_cast_fp16_to_uint16_dtype_0"), val = string("uint16")];
            uint16 gather_47_indices_0_to_uint16 = const()[name = string("gather_47_indices_0_to_uint16"), val = uint16(0)];
            tensor<uint16, [4]> var_517_shape_cast_fp16_to_uint16 = cast(dtype = var_517_shape_cast_fp16_to_uint16_dtype_0, x = var_517_shape_cast_fp16)[name = string("cast_2")];
            uint16 gather_47_cast_uint16 = gather(axis = gather_47_axis_0, batch_dims = gather_47_batch_dims_0, indices = gather_47_indices_0_to_uint16, validate_indices = gather_47_validate_indices_0, x = var_517_shape_cast_fp16_to_uint16)[name = string("gather_47_cast_uint16")];
            string gather_47_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_47_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 gather_48_axis_0 = const()[name = string("gather_48_axis_0"), val = int32(0)];
            int32 gather_48_batch_dims_0 = const()[name = string("gather_48_batch_dims_0"), val = int32(0)];
            bool gather_48_validate_indices_0 = const()[name = string("gather_48_validate_indices_0"), val = bool(false)];
            uint16 gather_48_indices_0_to_uint16 = const()[name = string("gather_48_indices_0_to_uint16"), val = uint16(1)];
            uint16 gather_48_cast_uint16 = gather(axis = gather_48_axis_0, batch_dims = gather_48_batch_dims_0, indices = gather_48_indices_0_to_uint16, validate_indices = gather_48_validate_indices_0, x = var_517_shape_cast_fp16_to_uint16)[name = string("gather_48_cast_uint16")];
            string gather_48_cast_uint16_to_int32_dtype_0 = const()[name = string("gather_48_cast_uint16_to_int32_dtype_0"), val = string("int32")];
            int32 concat_24_axis_0 = const()[name = string("concat_24_axis_0"), val = int32(0)];
            bool concat_24_interleave_0 = const()[name = string("concat_24_interleave_0"), val = bool(false)];
            int32 gather_48_cast_uint16_to_int32 = cast(dtype = gather_48_cast_uint16_to_int32_dtype_0, x = gather_48_cast_uint16)[name = string("cast_0")];
            int32 gather_47_cast_uint16_to_int32 = cast(dtype = gather_47_cast_uint16_to_int32_dtype_0, x = gather_47_cast_uint16)[name = string("cast_1")];
            tensor<int32, [3]> concat_24 = concat(axis = concat_24_axis_0, interleave = concat_24_interleave_0, values = (gather_47_cast_uint16_to_int32, gather_48_cast_uint16_to_int32, var_27))[name = string("concat_24")];
            tensor<fp16, [?, ?, 384]> input_125_cast_fp16 = reshape(shape = concat_24, x = var_515_cast_fp16)[name = string("input_125_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21371264))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21518784))))[name = string("model_encoder_layer_5_attention_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_5_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21519616)))];
            fp16 quantize_69_scale_0 = const()[name = string("quantize_69_scale_0"), val = fp16(nan)];
            string quantize_69_output_dtype_0 = const()[name = string("quantize_69_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_69 = quantize(input = input_125_cast_fp16, output_dtype = quantize_69_output_dtype_0, scale = quantize_69_scale_0)[name = string("quantize_69")];
            fp16 dequantize_69_scale_0 = const()[name = string("dequantize_69_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_69 = dequantize(input = quantize_69, scale = dequantize_69_scale_0)[name = string("dequantize_69")];
            tensor<fp16, [?, ?, 384]> linear_33_cast_fp16 = linear(bias = model_encoder_layer_5_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_5_attention_output_dense_weight_to_fp16_quantized, x = dequantize_69)[name = string("linear_33_cast_fp16")];
            fp16 quantize_70_scale_0 = const()[name = string("quantize_70_scale_0"), val = fp16(nan)];
            string quantize_70_output_dtype_0 = const()[name = string("quantize_70_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_70 = quantize(input = linear_33_cast_fp16, output_dtype = quantize_70_output_dtype_0, scale = quantize_70_scale_0)[name = string("quantize_70")];
            fp16 quantize_71_scale_0 = const()[name = string("quantize_71_scale_0"), val = fp16(nan)];
            string quantize_71_output_dtype_0 = const()[name = string("quantize_71_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_71 = quantize(input = input_119_cast_fp16, output_dtype = quantize_71_output_dtype_0, scale = quantize_71_scale_0)[name = string("quantize_71")];
            fp16 dequantize_202_scale_0 = const()[name = string("dequantize_202_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_202 = dequantize(input = quantize_71, scale = dequantize_202_scale_0)[name = string("dequantize_202")];
            fp16 dequantize_36_scale_0_1 = const()[name = string("dequantize_36_scale_0_1"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_36_1 = dequantize(input = quantize_70, scale = dequantize_36_scale_0_1)[name = string("dequantize_36_1")];
            tensor<fp16, [?, ?, 384]> input_129_cast_fp16 = add(x = dequantize_36_1, y = dequantize_202)[name = string("input_129_cast_fp16")];
            string quantize_117_output_dtype_0 = const()[name = string("quantize_117_output_dtype_0"), val = string("int8")];
            fp16 quantize_18_scale_0_1 = const()[name = string("quantize_18_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_18_1 = quantize(input = input_129_cast_fp16, output_dtype = quantize_117_output_dtype_0, scale = quantize_18_scale_0_1)[name = string("quantize_18_1")];
            fp16 dequantize_117_scale_0 = const()[name = string("dequantize_117_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_161 = dequantize(input = quantize_18_1, scale = dequantize_117_scale_0)[name = string("dequantize_161")];
            tensor<int32, [1]> input_131_axes_0 = const()[name = string("input_131_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21520448)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21521280)))];
            tensor<fp16, [?, ?, 384]> input_131_cast_fp16 = layer_norm(axes = input_131_axes_0, beta = model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16, x = dequantize_161)[name = string("input_131_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_5_intermediate_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [1536, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(21522112))), scale = tensor<fp16, [1536, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22112000))))[name = string("model_encoder_layer_5_intermediate_dense_weight_to_fp16_quantized")];
            tensor<fp16, [1536]> model_encoder_layer_5_intermediate_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_5_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22115136)))];
            fp16 quantize_72_scale_0 = const()[name = string("quantize_72_scale_0"), val = fp16(nan)];
            string quantize_72_output_dtype_0 = const()[name = string("quantize_72_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_72 = quantize(input = input_131_cast_fp16, output_dtype = quantize_72_output_dtype_0, scale = quantize_72_scale_0)[name = string("quantize_72")];
            fp16 dequantize_72_scale_0 = const()[name = string("dequantize_72_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_72 = dequantize(input = quantize_72, scale = dequantize_72_scale_0)[name = string("dequantize_72")];
            tensor<fp16, [?, ?, 1536]> linear_34_cast_fp16 = linear(bias = model_encoder_layer_5_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_5_intermediate_dense_weight_to_fp16_quantized, x = dequantize_72)[name = string("linear_34_cast_fp16")];
            fp16 quantize_118_scale_0 = const()[name = string("quantize_118_scale_0"), val = fp16(nan)];
            string quantize_118_output_dtype_0 = const()[name = string("quantize_118_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_162 = quantize(input = linear_34_cast_fp16, output_dtype = quantize_118_output_dtype_0, scale = quantize_118_scale_0)[name = string("quantize_162")];
            fp16 dequantize_118_scale_0 = const()[name = string("dequantize_118_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_162 = dequantize(input = quantize_162, scale = dequantize_118_scale_0)[name = string("dequantize_162")];
            string input_135_mode_0 = const()[name = string("input_135_mode_0"), val = string("EXACT")];
            tensor<fp16, [?, ?, 1536]> input_135_cast_fp16 = gelu(mode = input_135_mode_0, x = dequantize_162)[name = string("input_135_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_5_output_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 1536]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22118272))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22708160))))[name = string("model_encoder_layer_5_output_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_encoder_layer_5_output_dense_bias_to_fp16 = const()[name = string("model_encoder_layer_5_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22708992)))];
            fp16 quantize_73_scale_0 = const()[name = string("quantize_73_scale_0"), val = fp16(nan)];
            string quantize_73_output_dtype_0 = const()[name = string("quantize_73_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 1536]> quantize_73 = quantize(input = input_135_cast_fp16, output_dtype = quantize_73_output_dtype_0, scale = quantize_73_scale_0)[name = string("quantize_73")];
            fp16 dequantize_73_scale_0 = const()[name = string("dequantize_73_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 1536]> dequantize_73 = dequantize(input = quantize_73, scale = dequantize_73_scale_0)[name = string("dequantize_73")];
            tensor<fp16, [?, ?, 384]> linear_35_cast_fp16 = linear(bias = model_encoder_layer_5_output_dense_bias_to_fp16, weight = model_encoder_layer_5_output_dense_weight_to_fp16_quantized, x = dequantize_73)[name = string("linear_35_cast_fp16")];
            fp16 quantize_74_scale_0 = const()[name = string("quantize_74_scale_0"), val = fp16(nan)];
            string quantize_74_output_dtype_0 = const()[name = string("quantize_74_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_74 = quantize(input = linear_35_cast_fp16, output_dtype = quantize_74_output_dtype_0, scale = quantize_74_scale_0)[name = string("quantize_74")];
            fp16 quantize_75_scale_0 = const()[name = string("quantize_75_scale_0"), val = fp16(nan)];
            string quantize_75_output_dtype_0 = const()[name = string("quantize_75_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, ?, 384]> quantize_75 = quantize(input = input_131_cast_fp16, output_dtype = quantize_75_output_dtype_0, scale = quantize_75_scale_0)[name = string("quantize_75")];
            fp16 dequantize_204_scale_0 = const()[name = string("dequantize_204_scale_0"), val = fp16(0x1p+0)];
            tensor<fp16, [?, ?, 384]> dequantize_204 = dequantize(input = quantize_75, scale = dequantize_204_scale_0)[name = string("dequantize_204")];
            fp16 dequantize_38_scale_0 = const()[name = string("dequantize_38_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_38 = dequantize(input = quantize_74, scale = dequantize_38_scale_0)[name = string("dequantize_38")];
            tensor<fp16, [?, ?, 384]> input_139_cast_fp16 = add(x = dequantize_38, y = dequantize_204)[name = string("input_139_cast_fp16")];
            string quantize_119_output_dtype_0 = const()[name = string("quantize_119_output_dtype_0"), val = string("int8")];
            fp16 quantize_19_scale_0_1 = const()[name = string("quantize_19_scale_0_1"), val = fp16(nan)];
            tensor<int8, [?, ?, 384]> quantize_19_1 = quantize(input = input_139_cast_fp16, output_dtype = quantize_119_output_dtype_0, scale = quantize_19_scale_0_1)[name = string("quantize_19_1")];
            fp16 dequantize_119_scale_0 = const()[name = string("dequantize_119_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, ?, 384]> dequantize_163 = dequantize(input = quantize_19_1, scale = dequantize_119_scale_0)[name = string("dequantize_163")];
            tensor<int32, [1]> hidden_states_axes_0 = const()[name = string("hidden_states_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_weight_to_fp16 = const()[name = string("model_encoder_layer_5_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22709824)))];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_bias_to_fp16 = const()[name = string("model_encoder_layer_5_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22710656)))];
            tensor<fp16, [?, ?, 384]> hidden_states_cast_fp16 = layer_norm(axes = hidden_states_axes_0, beta = model_encoder_layer_5_output_LayerNorm_bias_to_fp16, epsilon = var_26_to_fp16, gamma = model_encoder_layer_5_output_LayerNorm_weight_to_fp16, x = dequantize_163)[name = string("hidden_states_cast_fp16")];
            tensor<int32, [3]> input_141_begin_0 = const()[name = string("input_141_begin_0"), val = tensor<int32, [3]>([0, 0, 0])];
            tensor<int32, [3]> input_141_end_0 = const()[name = string("input_141_end_0"), val = tensor<int32, [3]>([0, 1, 384])];
            tensor<bool, [3]> input_141_end_mask_0 = const()[name = string("input_141_end_mask_0"), val = tensor<bool, [3]>([true, false, true])];
            tensor<bool, [3]> input_141_squeeze_mask_0 = const()[name = string("input_141_squeeze_mask_0"), val = tensor<bool, [3]>([false, true, false])];
            tensor<fp16, [?, 384]> input_141_cast_fp16 = slice_by_index(begin = input_141_begin_0, end = input_141_end_0, end_mask = input_141_end_mask_0, squeeze_mask = input_141_squeeze_mask_0, x = hidden_states_cast_fp16)[name = string("input_141_cast_fp16")];
            tensor<fp16, [384, 384]> model_pooler_dense_weight_to_fp16_quantized = constexpr_blockwise_shift_scale(data = tensor<int8, [384, 384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22711488))), scale = tensor<fp16, [384, 1]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22859008))))[name = string("model_pooler_dense_weight_to_fp16_quantized")];
            tensor<fp16, [384]> model_pooler_dense_bias_to_fp16 = const()[name = string("model_pooler_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = string("@model_path/weights/weight.bin"), offset = uint64(22859840)))];
            fp16 quantize_76_scale_0 = const()[name = string("quantize_76_scale_0"), val = fp16(nan)];
            string quantize_76_output_dtype_0 = const()[name = string("quantize_76_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 384]> quantize_76 = quantize(input = input_141_cast_fp16, output_dtype = quantize_76_output_dtype_0, scale = quantize_76_scale_0)[name = string("quantize_76")];
            fp16 dequantize_76_scale_0 = const()[name = string("dequantize_76_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 384]> dequantize_76 = dequantize(input = quantize_76, scale = dequantize_76_scale_0)[name = string("dequantize_76")];
            tensor<fp16, [?, 384]> linear_36_cast_fp16 = linear(bias = model_pooler_dense_bias_to_fp16, weight = model_pooler_dense_weight_to_fp16_quantized, x = dequantize_76)[name = string("linear_36_cast_fp16")];
            fp16 quantize_120_scale_0 = const()[name = string("quantize_120_scale_0"), val = fp16(nan)];
            string quantize_120_output_dtype_0 = const()[name = string("quantize_120_output_dtype_0"), val = string("int8")];
            tensor<int8, [?, 384]> quantize_164 = quantize(input = linear_36_cast_fp16, output_dtype = quantize_120_output_dtype_0, scale = quantize_120_scale_0)[name = string("quantize_164")];
            fp16 dequantize_120_scale_0 = const()[name = string("dequantize_120_scale_0"), val = fp16(nan)];
            tensor<fp16, [?, 384]> dequantize_164 = dequantize(input = quantize_164, scale = dequantize_120_scale_0)[name = string("dequantize_164")];
            tensor<fp16, [?, 384]> var_554 = tanh(x = dequantize_164)[name = string("op_554_cast_fp16")];
        } -> (var_554);
}